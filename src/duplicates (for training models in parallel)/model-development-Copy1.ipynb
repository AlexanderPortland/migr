{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa6b5c",
   "metadata": {},
   "source": [
    "# Splitting, Preprocessing and Model Development\n",
    "This notebook is used for:\n",
    "\n",
    "\n",
    "### Declaring Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318f1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import sklearn\n",
    "import time\n",
    "import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a4beb",
   "metadata": {},
   "source": [
    "### Color Palette & Typeface Sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9835e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "YELLOW = '#F2DC5D'\n",
    "GREEN = '#9BC53D'\n",
    "DARK_GREEN = '#597222'\n",
    "RED = '#C3423F'\n",
    "LIGHT_BLUE = '#2596BE'\n",
    "GRAY = '#666666'\n",
    "\n",
    "AXIS_SIZE = 12\n",
    "TITLE_SIZE = 16\n",
    "DESCRIPTION_SIZE = 9\n",
    "FIGURE_SIZE = (10*2/3,6*2/3)\n",
    "\n",
    "RANDOM_STATE = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849d3c5",
   "metadata": {},
   "source": [
    "### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f2f082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citizen</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>geo</th>\n",
       "      <th>TIME_PERIOD</th>\n",
       "      <th>GENCONV</th>\n",
       "      <th>HUMSTAT</th>\n",
       "      <th>SUB_PROT</th>\n",
       "      <th>REJECTED</th>\n",
       "      <th>TOTAL_POS</th>\n",
       "      <th>TOTAL_APPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221109</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221110</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221111</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221112</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221113</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7213806 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citizen  sex     age geo TIME_PERIOD  GENCONV  HUMSTAT  SUB_PROT  \\\n",
       "0            AD    F     UNK  AT     2008-Q1        0        0         0   \n",
       "1            AD    F     UNK  AT     2008-Q2        0        0         0   \n",
       "2            AD    F     UNK  AT     2008-Q3        0        0         0   \n",
       "3            AD    F     UNK  AT     2008-Q4        0        0         0   \n",
       "4            AD    F     UNK  AT     2009-Q1        0        0         0   \n",
       "...         ...  ...     ...  ..         ...      ...      ...       ...   \n",
       "7221109      ZW  UNK  Y_LT14  UK     2019-Q3        0        0         0   \n",
       "7221110      ZW  UNK  Y_LT14  UK     2019-Q4        0        0         0   \n",
       "7221111      ZW  UNK  Y_LT14  UK     2020-Q1        0        0         0   \n",
       "7221112      ZW  UNK  Y_LT14  UK     2020-Q2        0        0         0   \n",
       "7221113      ZW  UNK  Y_LT14  UK     2020-Q3        0        0         0   \n",
       "\n",
       "         REJECTED  TOTAL_POS  TOTAL_APPS  \n",
       "0               0          0           0  \n",
       "1               0          0           0  \n",
       "2               0          0           0  \n",
       "3               0          0           0  \n",
       "4               0          0           0  \n",
       "...           ...        ...         ...  \n",
       "7221109         0          0           0  \n",
       "7221110         0          0           0  \n",
       "7221111         0          0           0  \n",
       "7221112         0          0           0  \n",
       "7221113         0          0           0  \n",
       "\n",
       "[7213806 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#****************************************************import dataset****************************************************\n",
    "df = pd.read_csv('../data/final.csv', dtype={'citizen': 'string', 'sex': 'string', 'age': 'string', 'decision': 'string', 'geo': 'string', 'TIME_PERIOD': 'string', 'GENCONV': \"Int64\", 'HUMSTAT': \"Int64\", 'SUB_PROT': \"Int64\", 'REJECTED': \"Int64\", 'TOTAL_POS': \"Int64\", 'TOTAL_APPS': \"Int64\", \"POS_RATE\": \"Float64\"}, keep_default_na=False, na_values=['nan'])\n",
    "\n",
    "##remove partial 2023-Q3 Data\n",
    "df = df[df[\"TIME_PERIOD\"] != \"2023-Q3\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500d090",
   "metadata": {},
   "source": [
    "## 1. Introduce Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5da77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citizen</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>geo</th>\n",
       "      <th>TIME_PERIOD</th>\n",
       "      <th>GENCONV</th>\n",
       "      <th>HUMSTAT</th>\n",
       "      <th>SUB_PROT</th>\n",
       "      <th>REJECTED</th>\n",
       "      <th>TOTAL_POS</th>\n",
       "      <th>TOTAL_APPS</th>\n",
       "      <th>TOTAL_POS - lag 1 quarter</th>\n",
       "      <th>TOTAL_POS - lag 2 quarters</th>\n",
       "      <th>TOTAL_POS - lag 3 quarters</th>\n",
       "      <th>TOTAL_POS - lag 4 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 1 quarter</th>\n",
       "      <th>TOTAL_APPS - lag 2 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 3 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 4 quarters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2010-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221109</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221110</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221111</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221112</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221113</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6731478 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citizen  sex     age geo TIME_PERIOD  GENCONV  HUMSTAT  SUB_PROT  \\\n",
       "4            AD    F     UNK  AT     2009-Q1        0        0         0   \n",
       "5            AD    F     UNK  AT     2009-Q2        0        0         0   \n",
       "6            AD    F     UNK  AT     2009-Q3        0        0         0   \n",
       "7            AD    F     UNK  AT     2009-Q4        0        0         0   \n",
       "8            AD    F     UNK  AT     2010-Q1        0        0         0   \n",
       "...         ...  ...     ...  ..         ...      ...      ...       ...   \n",
       "7221109      ZW  UNK  Y_LT14  UK     2019-Q3        0        0         0   \n",
       "7221110      ZW  UNK  Y_LT14  UK     2019-Q4        0        0         0   \n",
       "7221111      ZW  UNK  Y_LT14  UK     2020-Q1        0        0         0   \n",
       "7221112      ZW  UNK  Y_LT14  UK     2020-Q2        0        0         0   \n",
       "7221113      ZW  UNK  Y_LT14  UK     2020-Q3        0        0         0   \n",
       "\n",
       "         REJECTED  TOTAL_POS  TOTAL_APPS  TOTAL_POS - lag 1 quarter  \\\n",
       "4               0          0           0                          0   \n",
       "5               0          0           0                          0   \n",
       "6               0          0           0                          0   \n",
       "7               0          0           0                          0   \n",
       "8               0          0           0                          0   \n",
       "...           ...        ...         ...                        ...   \n",
       "7221109         0          0           0                          0   \n",
       "7221110         0          0           0                          0   \n",
       "7221111         0          0           0                          0   \n",
       "7221112         0          0           0                          0   \n",
       "7221113         0          0           0                          0   \n",
       "\n",
       "         TOTAL_POS - lag 2 quarters  TOTAL_POS - lag 3 quarters  \\\n",
       "4                                 0                           0   \n",
       "5                                 0                           0   \n",
       "6                                 0                           0   \n",
       "7                                 0                           0   \n",
       "8                                 0                           0   \n",
       "...                             ...                         ...   \n",
       "7221109                           0                           0   \n",
       "7221110                           0                           0   \n",
       "7221111                           0                           0   \n",
       "7221112                           0                           0   \n",
       "7221113                           0                           0   \n",
       "\n",
       "         TOTAL_POS - lag 4 quarters  TOTAL_APPS - lag 1 quarter  \\\n",
       "4                                 0                           0   \n",
       "5                                 0                           0   \n",
       "6                                 0                           0   \n",
       "7                                 0                           0   \n",
       "8                                 0                           0   \n",
       "...                             ...                         ...   \n",
       "7221109                           0                           0   \n",
       "7221110                           0                           0   \n",
       "7221111                           0                           0   \n",
       "7221112                           0                           0   \n",
       "7221113                           0                           0   \n",
       "\n",
       "         TOTAL_APPS - lag 2 quarters  TOTAL_APPS - lag 3 quarters  \\\n",
       "4                                  0                            0   \n",
       "5                                  0                            0   \n",
       "6                                  0                            0   \n",
       "7                                  0                            0   \n",
       "8                                  0                            0   \n",
       "...                              ...                          ...   \n",
       "7221109                            0                            0   \n",
       "7221110                            0                            0   \n",
       "7221111                            0                            0   \n",
       "7221112                            0                            0   \n",
       "7221113                            0                            0   \n",
       "\n",
       "         TOTAL_APPS - lag 4 quarters  \n",
       "4                                  0  \n",
       "5                                  0  \n",
       "6                                  0  \n",
       "7                                  0  \n",
       "8                                  0  \n",
       "...                              ...  \n",
       "7221109                            0  \n",
       "7221110                            0  \n",
       "7221111                            0  \n",
       "7221112                            0  \n",
       "7221113                            0  \n",
       "\n",
       "[6731478 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#****************************************************re-sort dataframe****************************************************\n",
    "\n",
    "sort_order = ['citizen', 'sex', 'age', 'geo', 'TIME_PERIOD']\n",
    "df = df.sort_values(by =sort_order) \n",
    "\n",
    "#*********************************************create sequential list of quarters****************************************************\n",
    "\n",
    "quarters = []\n",
    "for i in range(2008, 2024):\n",
    "    quarters.append(str(i) + \"-Q1\")\n",
    "    quarters.append(str(i) + \"-Q2\")\n",
    "    quarters.append(str(i) + \"-Q3\")\n",
    "    quarters.append(str(i) + \"-Q4\")\n",
    "\n",
    "#****************************************************lagged features****************************************************\n",
    "\n",
    "QUARTERS_OF_LAG = (4 * 1)\n",
    "\n",
    "def add_lagged_features(df, features, maintained_columns, QUARTERS_OF_LAG):\n",
    "    quarters = np.unique(df[\"TIME_PERIOD\"])\n",
    "    def lagged_features(target_var, lag_count, unit):\n",
    "        lagged = pd.DataFrame()\n",
    "        columns = []\n",
    "        for i in range(1, lag_count + 1):\n",
    "            lagged = pd.concat([lagged, target_var.shift(i)], axis=1)\n",
    "            name = target_var.name\n",
    "            if (i == 1):\n",
    "                columns.append(name + \" - lag \" + str(i) + \" \" + str(unit))\n",
    "            else:\n",
    "                columns.append(name + \" - lag \" + str(i) + \" \" + str(unit) + \"s\")\n",
    "        lagged.columns = columns\n",
    "        return lagged.astype('Int64')\n",
    "\n",
    "    #introduce lag for each feature\n",
    "    df_lagged = df\n",
    "    for f in features:\n",
    "        df_lagged = pd.concat([df_lagged, lagged_features(df[f], QUARTERS_OF_LAG, \"quarter\")], axis=1)\n",
    "\n",
    "    #remove all features with less than the lag amount of historical data\n",
    "    #df_lagged = df_lagged[df_lagged.eq()]\n",
    "    for i in range(0, QUARTERS_OF_LAG):\n",
    "        shift_eq = df_lagged.eq(df_lagged.shift())\n",
    "        keep = shift_eq[maintained_columns[0]]\n",
    "        for j in range(1, len(maintained_columns)):\n",
    "            keep = keep & shift_eq[maintained_columns[j]]\n",
    "        df_lagged = df_lagged[keep]\n",
    "        #print(\"lagged i\" + str(i) + \" of \" + str(QUARTERS_OF_LAG))\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "df_lagged = add_lagged_features(df, [\"TOTAL_POS\", \"TOTAL_APPS\"], ['citizen', 'age', 'sex', 'geo'], QUARTERS_OF_LAG)\n",
    "\n",
    "df_lagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e5062",
   "metadata": {},
   "source": [
    "### Lagged Feature Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cfc60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] testing lagging function\n",
      "\t[+] basic lag tests passed\n",
      "\t[+] tests with shared times passed\n",
      "\t[+] tests with multiple start and end dates passed\n",
      "\t[+] \u001b[42mPASSED ALL\u001b[0m lagging tests\n"
     ]
    }
   ],
   "source": [
    "#***********************************************testing equivalence function****************************************************\n",
    "def equivalent_dfs(df1, df2):\n",
    "    return (df1.reset_index(drop=True) == df2.reset_index(drop=True)).all().all()\n",
    "\n",
    "#****************************************************lagging testing****************************************************\n",
    "print(\"[*] testing lagging function\")\n",
    "#simple tests\n",
    "tdf0_data = [['AR', 1, 1], ['AR', 2, 2], ['AR', 3, 3], ['AR', 4, 4], ['AR', 5, 5]]\n",
    "tdf0 = pd.DataFrame(data=tdf0_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf1_data = [['AR', 2, 2, 1], ['AR', 3, 3, 2], ['AR', 4, 4, 3], ['AR', 5, 5, 4]]\n",
    "tdf1 = pd.DataFrame(data=tdf1_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf2_data = [['AR', 3, 3, 2, 1], ['AR', 4, 4, 3, 2], ['AR', 5, 5, 4, 3]]\n",
    "tdf2 = pd.DataFrame(data=tdf2_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "tdf3_data = [['AR', 4, 4, 3, 2, 1], ['AR', 5, 5, 4, 3, 2]]\n",
    "tdf3 = pd.DataFrame(data=tdf3_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters', 'val - lag 3 quarters'])\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 1), \n",
    "                      tdf1), \"basic lag test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 2), \n",
    "                      tdf2), \"basic lag test failed, shift=2\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 3), \n",
    "                      tdf3), \"basic lag test failed, shift=3\"\n",
    "print(\"\\t[+] basic lag tests passed\")\n",
    "\n",
    "#more advanced tests\n",
    "tdf4_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "             ['NZ', 1, 11], ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15]]\n",
    "tdf4 = pd.DataFrame(data=tdf4_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf5_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "             ['NZ', 2, 12, 11], ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14]]\n",
    "tdf5 = pd.DataFrame(data=tdf5_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf6_data = [['AU', 3, 3, 2, 1], ['AU', 4, 4, 3, 2], ['AU', 5, 5, 4, 3],\n",
    "             ['NZ', 3, 13, 12, 11], ['NZ', 4, 14, 13, 12], ['NZ', 5, 15, 14, 13]]\n",
    "tdf6 = pd.DataFrame(data=tdf6_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf4, [\"val\"], ['citizen'], 1), \n",
    "                      tdf5), \"lag with shared times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf4, [\"val\"], ['citizen'], 2), \n",
    "                      tdf6), \"lag with shared times test failed, shift=2\"\n",
    "print(\"\\t[+] tests with shared times passed\")\n",
    "\n",
    "#very complicated tests\n",
    "tdf7_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "             ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15]]\n",
    "tdf7 = pd.DataFrame(data=tdf7_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf8_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "             ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14]]\n",
    "tdf8 = pd.DataFrame(data=tdf8_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf9_data = [['AU', 3, 3, 2, 1], ['AU', 4, 4, 3, 2], ['AU', 5, 5, 4, 3],\n",
    "             ['NZ', 4, 14, 13, 12], ['NZ', 5, 15, 14, 13]]\n",
    "tdf9 = pd.DataFrame(data=tdf9_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "tdf10_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "              ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15],\n",
    "              ['FJ', 3, 23], ['FJ', 4, 24], ['FJ', 5, 25], ['FJ', 6, 26], ['FJ', 7, 27], ['FJ', 8, 28],\n",
    "              ['WS', 4, 34], ['WS', 5, 35]]\n",
    "tdf10 = pd.DataFrame(data=tdf10_data, columns=['citizen', 'TIME_PERIOD', 'new_val'])\n",
    "tdf11_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "              ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14],\n",
    "              ['FJ', 4, 24, 23], ['FJ', 5, 25, 24], ['FJ', 6, 26, 25], ['FJ', 7, 27, 26], ['FJ', 8, 28, 27],\n",
    "              ['WS', 5, 35, 34]]\n",
    "tdf11 = pd.DataFrame(data=tdf11_data, columns=['citizen', 'TIME_PERIOD', 'new_val', 'new_val - lag 1 quarter'])\n",
    "tdf12_data = [['AU', 5, 5, 4, 3, 2, 1],\n",
    "              ['FJ', 7, 27, 26, 25, 24, 23], ['FJ', 8, 28, 27, 26, 25, 24]]\n",
    "tdf12 = pd.DataFrame(data=tdf12_data, columns=['citizen', 'TIME_PERIOD', 'new_val', 'new_val - lag 1 quarter', 'new_val - lag 2 quarters', 'new_val - lag 3 quarters', 'new_val - lag 4 quarters'])\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf7, [\"val\"], ['citizen'], 1), \n",
    "                      tdf8), \"lag with 2 different start times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf7, [\"val\"], ['citizen'], 2), \n",
    "                      tdf9), \"lag with 2 different start times test failed, shift=2\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf10, [\"new_val\"], ['citizen'], 1), \n",
    "                      tdf11), \"lag with many different start times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf10, [\"new_val\"], ['citizen'], 4), \n",
    "                      tdf12), \"lag with many different start times test failed, shift=4\"\n",
    "print(\"\\t[+] tests with multiple start and end dates passed\")\n",
    "\n",
    "print(\"\\t[+] \\x1b[42mPASSED ALL\\x1b[0m lagging tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd114121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d6700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb22ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1acfa54",
   "metadata": {},
   "source": [
    "## 2. Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ace99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************splitting****************************************************\n",
    "TARGET_VAR = \"TOTAL_POS\"\n",
    "\n",
    "y = df_lagged[TARGET_VAR]\n",
    "X = df_lagged.drop(['GENCONV', 'HUMSTAT', 'SUB_PROT', 'REJECTED', 'TOTAL_POS'], axis=1)\n",
    "#PLAN:\n",
    "#of 62 quarters...\n",
    "#QUARTERS_OF_LAG are lost bc they wont have the needed lagged features\n",
    "    \n",
    "new_quarters = [q for q in quarters if q >= quarters[QUARTERS_OF_LAG]]\n",
    "quarter_count = len(new_quarters) - 1\n",
    "\n",
    "TRAIN_PORTION = 0.6\n",
    "VAL_PORTION = 0.2\n",
    "TEST_PORTION = 0.2\n",
    "\n",
    "#take out last portion of quarters for testing\n",
    "#div_0 = new_quarters[0]\n",
    "#div_1 = new_quarters[int(quarter_count * train_split)]\n",
    "div_2 = new_quarters[int(quarter_count * (1 - TEST_PORTION))]\n",
    "div_3 = new_quarters[quarter_count]\n",
    "\n",
    "#seperate out test section\n",
    "X_test = X[(div_2 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_3)]\n",
    "y_test = y[(div_2 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442d852",
   "metadata": {},
   "source": [
    "### Evaluation Functions (for cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38feb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).sum() / len(y_pred))\n",
    "\n",
    "def BASELINE_RMSE(y_true):\n",
    "    return ALL_ZERO_BASELINE(y_true)\n",
    "\n",
    "def ALL_ZERO_BASELINE(y_true):\n",
    "    return np.sqrt(((y_true) ** 2).sum() / len(y_true))\n",
    "\n",
    "def ALL_ONE_BASELINE(y_true):\n",
    "    return np.sqrt(((1 - y_true) ** 2).sum() / len(y_true))\n",
    "\n",
    "def ALL_MEAN_BASELINE(y_train, y_true):\n",
    "    mean = y_train.mean()\n",
    "    return np.sqrt(((mean - y_true) ** 2).sum() / len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656b3c7",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b96604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIMESTAMP_STR():\n",
    "    dt = datetime.datetime.now()\n",
    "    txt = \"{hour}:{minute:02.0f} on {day}-{month}-{year}\"\n",
    "    return txt.format(hour=dt.hour, minute=dt.minute, day=dt.day, month=dt.month, year=dt.year)\n",
    "\n",
    "def DF_ALL_PREDICTED(model):\n",
    "    df_new = df\n",
    "    df_new[\"TOTAL_APPS_PRED\"] = model.predict(X)\n",
    "\n",
    "UPDATE = True\n",
    "def Status_Update(t, message):\n",
    "    my_time = time.time()\n",
    "    if UPDATE:\n",
    "        print(str(message) + \" \\t-- in \" + str(time.time() - t) + \"s\")\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e59817",
   "metadata": {},
   "source": [
    "### General Model-Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af9426eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPipe_TimeSeries_RMSE(X, y, preprocessor, ML_algo, param_grid):\n",
    "    ALGO_NAME = str(ML_algo)[str(ML_algo).rindex('.')+1:-2]\n",
    "    MODEL_NAME = \"{algo}({time})\".format(algo=ALGO_NAME, time=TIMESTAMP_STR())\n",
    "    NUM_FOLDS = np.round(TRAIN_PORTION / VAL_PORTION).astype(int)\n",
    "    RANDOM_STATE = 14\n",
    "    print(\"[!] looking at model: \" + str(MODEL_NAME))\n",
    "    \n",
    "    print(\"\\t[*] doing initial dataset splitting\")\n",
    "    \n",
    "    #opt check 2\n",
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    X_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(1, NUM_FOLDS + 1):\n",
    "        train_portion = np.round(i * VAL_PORTION, 2)\n",
    "        div_0 = new_quarters[0]\n",
    "        div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "        div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "        X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        print(\"\\t\\t[+] fold \" + str(i))\n",
    "    \n",
    "    param_scores = []\n",
    "    param_models = []\n",
    "    for p in param_grid:\n",
    "        T_PARAM_START = time.time()\n",
    "        fold_scores = []\n",
    "        print(\"\\t[*] looking at hyperparameters \" + str(p))\n",
    "        for i in range(NUM_FOLDS, NUM_FOLDS + 1):\n",
    "            T_FOLD_START = time.time()\n",
    "            #ADD OPTIMIZATION HERE:\n",
    "                #[2] NO NEED TO CONSTANTLY REDIVIDE THE DATA, SAME THREE FOLDS WILL BE USED EACH TIME REGARDLES OF HYPERPARAMAETERS\n",
    "                #[1] NO NEED TO CHECK FUTURE FOLDS IF EVEN A SCORE OF ZERO WOULDN'T BRING IT BELOW A BETTER FOUND PARAMETER ARRANGEMENT\n",
    "            train_portion = np.round(i * VAL_PORTION, 2)\n",
    "            \n",
    "            #opt check 1\n",
    "            if (i > 0 & len(param_scores) > 0):\n",
    "                best_param_score = param_scores[np.argmin(param_scores)]\n",
    "                if ((best_param_score * 3) < np.sum(fold_scores)):\n",
    "                    print(\"[!] best average score I could get with \" + str(np.sum(fold_scores)) + \" score already is worse than a previous params score of \" + str(best_param_score) + \", so giving up\")\n",
    "\n",
    "            mini_t = time.time()\n",
    "            \n",
    "            X_train = X_trains[i - 1]\n",
    "            y_train = y_trains[i - 1]\n",
    "            X_val = X_vals[i - 1]\n",
    "            y_val = y_vals[i - 1]\n",
    "            \n",
    "            PORTION_OF_POINTS = 0.1\n",
    "            X_train = X_train.sample(np.floor((len(X_train) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            y_train = y_train.loc[X_train.index]\n",
    "            X_val = X_val.sample(np.floor((len(X_val) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            y_val = y_val.loc[X_val.index]\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"split data\")\n",
    "\n",
    "            #make pipeline\n",
    "            if ((ALGO_NAME == \"SVR\") | (ALGO_NAME == \"LinearRegression\") | (ALGO_NAME == \"RandomForestRegressor\")):\n",
    "                algo = ML_algo(**p, verbose=1)\n",
    "            else:\n",
    "                algo = ML_algo(**p, random_state = RANDOM_STATE)\n",
    "            \n",
    "            #print(algo)\n",
    "            pipe = Pipeline(steps=[\n",
    "                        ('preprocess', preprocessor),\n",
    "                        ('model', algo)\n",
    "                    ], verbose=True)\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"made pipeline\")\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            mini_t = Status_Update(mini_t, \"fit pipeline\")\n",
    "            \n",
    "            y_pred = pipe.predict(X_val)\n",
    "            score = RMSE(y_pred, y_val)\n",
    "            \n",
    "            fold_scores.append(score)\n",
    "            T_FOLD_END = time.time()\n",
    "            T_FOLD_ELAPSED = T_FOLD_END - T_FOLD_START\n",
    "            mini_t = Status_Update(mini_t, \"eval pipeline\")\n",
    "            print(\"\\t\\t[*] fold \" + str(i) + \" complete (test score of \" + str(score) + \") -- in \" + str(np.round(T_FOLD_ELAPSED, 3)) + \"s\")\n",
    "        score = np.mean(fold_scores)\n",
    "        param_scores.append(score)\n",
    "        param_models.append(pipe)\n",
    "        \n",
    "        T_PARAM_END = time.time()\n",
    "        T_PARAM_ELAPSED = T_PARAM_END - T_PARAM_START\n",
    "        print(\"\\t\\t[+] final score for params of \" + str(score) + \" -- in \" + str(np.round(T_PARAM_ELAPSED, 3)) + \"s\")\n",
    "        \n",
    "        if (np.argmin(param_scores) == (len(param_scores) - 1)):\n",
    "            best_model = param_models[np.argmin(param_scores)]\n",
    "            print(\"\\t\\t[!] new best param configuration, so saving model to path\")\n",
    "            path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "            joblib.dump(best_model, path, compress = 1)\n",
    "            print(\"\\t\\t\\t[+] saved model to \" + str(path))\n",
    "        \n",
    "        \n",
    "    i_best = np.argmin(param_scores)\n",
    "    best_score = param_scores[i_best]\n",
    "    best_params = param_grid[i_best]\n",
    "    best_model = param_models[i_best]\n",
    "    print(\"\\t[+] best param configuration of \" + str(best_params) + \" found with score \" + str(best_score))\n",
    "    path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "    joblib.dump(best_model, path, compress = 1)\n",
    "    print(\"\\t[+] saved model to \" + str(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896b4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN_XGBOOST(X, y, preprocessor, ML_algo, param_grid):\n",
    "    ALGO_NAME = str(ML_algo)[str(ML_algo).rindex('.')+1:-2]\n",
    "    MODEL_NAME = \"{algo}({time})\".format(algo=ALGO_NAME, time=TIMESTAMP_STR())\n",
    "    NUM_FOLDS = np.round(TRAIN_PORTION / VAL_PORTION).astype(int)\n",
    "    RANDOM_STATE = 14\n",
    "    print(\"[!] looking at XGBoost model: \" + str(MODEL_NAME))\n",
    "    \n",
    "    print(\"\\t[*] doing initial dataset splitting\")\n",
    "    \n",
    "    #opt check 2\n",
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    X_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(1, NUM_FOLDS + 1):\n",
    "        train_portion = np.round(i * VAL_PORTION, 2)\n",
    "        div_0 = new_quarters[0]\n",
    "        div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "        div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "        X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        print(\"\\t\\t[+] fold \" + str(i))\n",
    "    \n",
    "    param_scores = []\n",
    "    param_models = []\n",
    "    for p in param_grid:\n",
    "        T_PARAM_START = time.time()\n",
    "        fold_scores = []\n",
    "        print(\"\\t[*] looking at hyperparameters \" + str(p))\n",
    "        for i in range(1, NUM_FOLDS + 1):\n",
    "            T_FOLD_START = time.time()\n",
    "            #ADD OPTIMIZATION HERE:\n",
    "                #[2] NO NEED TO CONSTANTLY REDIVIDE THE DATA, SAME THREE FOLDS WILL BE USED EACH TIME REGARDLES OF HYPERPARAMAETERS\n",
    "                #[1] NO NEED TO CHECK FUTURE FOLDS IF EVEN A SCORE OF ZERO WOULDN'T BRING IT BELOW A BETTER FOUND PARAMETER ARRANGEMENT\n",
    "            train_portion = np.round(i * VAL_PORTION, 2)\n",
    "            \n",
    "            #opt check 1\n",
    "            if (i > 0 & len(param_scores) > 0):\n",
    "                best_param_score = param_scores[np.argmin(param_scores)]\n",
    "                if ((best_param_score * 3) < np.sum(fold_scores)):\n",
    "                    print(\"[!] best average score I could get with \" + str(np.sum(fold_scores)) + \" score already is worse than a previous params score of \" + str(best_param_score) + \", so giving up\")\n",
    "\n",
    "            mini_t = time.time()\n",
    "            \n",
    "            X_train = X_trains[i - 1]\n",
    "            y_train = y_trains[i - 1]\n",
    "            X_val = X_vals[i - 1]\n",
    "            y_val = y_vals[i - 1]\n",
    "            \n",
    "            PORTION_OF_POINTS = 0.1\n",
    "            X_train = X_train.sample(np.floor((len(X_train) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            y_train = y_train.loc[X_train.index]\n",
    "            X_val = X_val.sample(np.floor((len(X_val) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            y_val = y_val.loc[X_val.index]\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"split data\")\n",
    "            \n",
    "            algo = ML_algo(**p, early_stopping_rounds=50)\n",
    "            \n",
    "            #print(algo)\n",
    "            mini_t = Status_Update(mini_t, \"made pipeline\")\n",
    "                \n",
    "            X_train = preprocessor.fit_transform(X_train)\n",
    "            X_val = preprocessor.transform(X_val)\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"preprocess data\")\n",
    "\n",
    "            algo.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "            mini_t = Status_Update(mini_t, \"fit pipeline\")\n",
    "            \n",
    "            y_pred = algo.predict(X_val)\n",
    "            score = RMSE(y_pred, y_val)\n",
    "            \n",
    "            fold_scores.append(score)\n",
    "            T_FOLD_END = time.time()\n",
    "            T_FOLD_ELAPSED = T_FOLD_END - T_FOLD_START\n",
    "            mini_t = Status_Update(mini_t, \"eval pipeline\")\n",
    "            print(\"\\t\\t[*] fold \" + str(i) + \" complete (test score of \" + str(score) + \") -- in \" + str(np.round(T_FOLD_ELAPSED, 3)) + \"s\")\n",
    "        score = np.mean(fold_scores)\n",
    "        param_scores.append(score)\n",
    "        param_models.append(algo)\n",
    "        \n",
    "        T_PARAM_END = time.time()\n",
    "        T_PARAM_ELAPSED = T_PARAM_END - T_PARAM_START\n",
    "        print(\"\\t\\t[+] final score for params of \" + str(score) + \" -- in \" + str(np.round(T_PARAM_ELAPSED, 3)) + \"s\")\n",
    "        \n",
    "        path = \"../results/\" + MODEL_NAME + '[seed ' + str(p['seed']) + \"].pkl\"\n",
    "        joblib.dump(algo, path, compress = 1)\n",
    "        print(\"\\t\\t\\t[+] saved model to \" + str(path))\n",
    "        #if (np.argmin(param_scores) == (len(param_scores) - 1)):\n",
    "            #best_model = param_models[np.argmin(param_scores)]\n",
    "            #print(\"\\t\\t[!] new best param configuration, so saving model to path\")\n",
    "            #path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "            #joblib.dump(best_model, path, compress = 1)\n",
    "            #print(\"\\t\\t\\t[+] saved model to \" + str(path))\n",
    "        \n",
    "        \n",
    "    i_best = np.argmin(param_scores)\n",
    "    best_score = param_scores[i_best]\n",
    "    best_params = param_grid[i_best]\n",
    "    best_model = param_models[i_best]\n",
    "    print(\"\\t[+] best param configuration of \" + str(best_params) + \" found with score \" + str(best_score))\n",
    "    path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "    joblib.dump(best_model, path, compress = 1)\n",
    "    print(\"\\t[+] saved model to \" + str(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0e896",
   "metadata": {},
   "source": [
    "### Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce777ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#MODELS TO TEST:\n",
    "#[Q] ~LASSO\n",
    "#[T] RIDGE\n",
    "#[] RIDGE (poly)\n",
    "#[DIED] ~ELASTIC NET\n",
    "#[T/DIED] RF\n",
    "#[T] SVR (linear)\n",
    "#[] ~SVR (rbf) ^if linear doesnt work\n",
    "#[IP] XGBoost\n",
    "#[] ~KNN??\n",
    "\n",
    "#T: Trained\n",
    "#IP: In Progress\n",
    "#Q: Queued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f2f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b318aa5c",
   "metadata": {},
   "source": [
    "### Training Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a9e64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m LASSO_PARAM_GRID \u001b[38;5;241m=\u001b[39m ParameterGrid({\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mgeomspace(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m12\u001b[39m), \u001b[38;5;66;03m# no upper bound so the values are evenly spaced in log\u001b[39;00m\n\u001b[1;32m      3\u001b[0m             })\n\u001b[0;32m----> 5\u001b[0m MLPipe_TimeSeries_RMSE(X, y, preprocessor, Lasso, LASSO_PARAM_GRID)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "LASSO_PARAM_GRID = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, Lasso, LASSO_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_PARAM_GRID = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "RIDGE_PARAM_GRID_SMALL = ParameterGrid({\n",
    "            'alpha': [0.1, 1, 10, 100], # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, Ridge, RIDGE_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_PARAM_GRID = {\n",
    "            'model__alpha': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000], # no upper bound so the values are evenly spaced in log\n",
    "            'model__l1_ratio': np.linspace(0.2, 1, 9)\n",
    "            }\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, ElasticNet, ELASTIC_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7028f2",
   "metadata": {},
   "source": [
    "### Training Non-Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_PARAM_GRID = ParameterGrid({\n",
    "            'max_depth': [1, 3, 4, 7, 10, 14, 21, 31], # no upper bound so the values are evenly spaced in log\n",
    "            'max_features': np.linspace(0.2, 1, 5)\n",
    "            })\n",
    "\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, RandomForestRegressor, RF_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56c4f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON #6\n",
      "[!] looking at model: RandomForestRegressor(23:57 on 4-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'n_jobs': -1, 'random_state': 6}\n",
      "split data \t-- in 0.7893466949462891s\n",
      "made pipeline \t-- in 0.00019621849060058594s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.8min\n",
      "fit pipeline \t-- in 170.06888008117676s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval pipeline \t-- in 0.2684929370880127s\n",
      "\t\t[*] fold 3 complete (test score of 9.457824888995576) -- in 171.127s\n",
      "\t\t[+] final score for params of 9.457824888995576 -- in 171.127s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(23:57 on 4-12-2023).pkl\n",
      "\t[+] best param configuration of {'random_state': 6, 'n_jobs': -1, 'max_depth': 10} found with score 9.457824888995576\n",
      "\t[+] saved model to ../results/RandomForestRegressor(23:57 on 4-12-2023).pkl\n",
      "WORKING ON #7\n",
      "[!] looking at model: RandomForestRegressor(0:00 on 5-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'n_jobs': -1, 'random_state': 7}\n",
      "split data \t-- in 0.8058028221130371s\n",
      "made pipeline \t-- in 0.00012373924255371094s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.8min\n",
      "fit pipeline \t-- in 168.85464191436768s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval pipeline \t-- in 0.42435288429260254s\n",
      "\t\t[*] fold 3 complete (test score of 9.379702760684944) -- in 170.085s\n",
      "\t\t[+] final score for params of 9.379702760684944 -- in 170.085s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(0:00 on 5-12-2023).pkl\n",
      "\t[+] best param configuration of {'random_state': 7, 'n_jobs': -1, 'max_depth': 10} found with score 9.379702760684944\n",
      "\t[+] saved model to ../results/RandomForestRegressor(0:00 on 5-12-2023).pkl\n",
      "WORKING ON #8\n",
      "[!] looking at model: RandomForestRegressor(0:02 on 5-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'n_jobs': -1, 'random_state': 8}\n",
      "split data \t-- in 0.8836770057678223s\n",
      "made pipeline \t-- in 0.0001609325408935547s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.8min\n",
      "fit pipeline \t-- in 169.83063983917236s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval pipeline \t-- in 0.30136609077453613s\n",
      "\t\t[*] fold 3 complete (test score of 8.229406426691497) -- in 171.016s\n",
      "\t\t[+] final score for params of 8.229406426691497 -- in 171.016s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(0:02 on 5-12-2023).pkl\n",
      "\t[+] best param configuration of {'random_state': 8, 'n_jobs': -1, 'max_depth': 10} found with score 8.229406426691497\n",
      "\t[+] saved model to ../results/RandomForestRegressor(0:02 on 5-12-2023).pkl\n"
     ]
    }
   ],
   "source": [
    "for i in [6, 7, 8]:\n",
    "    print(\"WORKING ON #\" + str(i))\n",
    "    RF_PARAM_GRID_VERY_SMALL = ParameterGrid({\n",
    "            'max_depth': [10], # no upper bound so the values are evenly spaced in log\n",
    "            #'max_features': [1],\n",
    "            'random_state': [i],\n",
    "            'n_jobs': [-1]\n",
    "            })\n",
    "    MLPipe_TimeSeries_RMSE(X, y, preprocessor, RandomForestRegressor, RF_PARAM_GRID_VERY_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c69e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_PARAM_GRID_POLY = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            #'normalize': [True]\n",
    "            })\n",
    "EMPTY = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            #'normalize': [True]\n",
    "            })\n",
    "\n",
    "\n",
    "#MLPipe_TimeSeries_RMSE(X, y, preprocessor_poly, Ridge, RIDGE_PARAM_GRID_POLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36f43579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "#trying hyperparameter space\n",
    "\n",
    "XGB_PARAM_GRID = ParameterGrid({\n",
    "    \"learning_rate\": [0.03],\n",
    "    \"n_estimators\": [1000],\n",
    "    #\"early_stopping_rounds\": [50],\n",
    "    \"reg_alpha\": [1e0, 1e1, 1e-1, 0, 1e-2, 1e2],\n",
    "    \"reg_lambda\": [1e0, 1e1, 1e-1, 0, 1e-2, 1e2],\n",
    "    \"seed\": [1956, 14, 3],\n",
    "    #\"max_depth\": [1,3,10,30,100],\n",
    "    \"colsample_bytree\": [0.9],              \n",
    "    \"subsample\": [0.66]\n",
    "    })\n",
    "\n",
    "#TRAIN_XGBOOST(X, y, preprocessor, xgboost.XGBRegressor, XGB_PARAM_GRID)\n",
    "#for p in XGB_PARAM_GRID:\n",
    "#    print(p['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "496919be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] looking at XGBoost model: XGBRegressor(23:46 on 4-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'n_estimators': 1000, 'reg_alpha': 1, 'reg_lambda': 1, 'seed': 3, 'subsample': 0.66}\n",
      "split data \t-- in 0.34569311141967773s\n",
      "made pipeline \t-- in 0.0001308917999267578s\n",
      "preprocess data \t-- in 0.4373948574066162s\n",
      "fit pipeline \t-- in 15.372934818267822s\n",
      "eval pipeline \t-- in 0.10110116004943848s\n",
      "\t\t[*] fold 1 complete (test score of 5.172200676308352) -- in 16.258s\n",
      "split data \t-- in 0.6933119297027588s\n",
      "made pipeline \t-- in 0.00019311904907226562s\n",
      "preprocess data \t-- in 0.6130340099334717s\n",
      "fit pipeline \t-- in 45.01893997192383s\n",
      "eval pipeline \t-- in 0.2608010768890381s\n",
      "\t\t[*] fold 2 complete (test score of 32.93696986937442) -- in 46.587s\n",
      "split data \t-- in 0.9371380805969238s\n",
      "made pipeline \t-- in 0.0003750324249267578s\n",
      "preprocess data \t-- in 1.0087580680847168s\n",
      "fit pipeline \t-- in 11.46664023399353s\n",
      "eval pipeline \t-- in 0.06878304481506348s\n",
      "\t\t[*] fold 3 complete (test score of 9.773026404627894) -- in 13.482s\n",
      "\t\t[+] final score for params of 15.96073231677022 -- in 76.326s\n",
      "\t\t\t[+] saved model to ../results/XGBRegressor(23:46 on 4-12-2023)[seed 3].pkl\n",
      "\t[+] best param configuration of {'subsample': 0.66, 'seed': 3, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 1000, 'learning_rate': 0.03, 'colsample_bytree': 0.9} found with score 15.96073231677022\n",
      "\t[+] saved model to ../results/XGBRegressor(23:46 on 4-12-2023).pkl\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "for i in [3]:\n",
    "    XGB_PARAM_GRID = ParameterGrid({\n",
    "        \"learning_rate\": [0.03],\n",
    "        \"n_estimators\": [1000],\n",
    "        #\"early_stopping_rounds\": [50],\n",
    "        \"reg_alpha\": [1],\n",
    "        \"reg_lambda\": [1],\n",
    "        \"seed\": [i],\n",
    "        #\"max_depth\": [1,3,10,30,100],\n",
    "        \"colsample_bytree\": [0.9],              \n",
    "        \"subsample\": [0.66]\n",
    "        })\n",
    "\n",
    "    TRAIN_XGBOOST(X, y, preprocessor, xgboost.XGBRegressor, XGB_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_partial = X_test.sample(np.floor((len(X_test) * 0.3)).astype(int))\n",
    "y_test_partial = y_test.loc[X_test_partial.index]\n",
    "y_test_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = []\n",
    "y_trains = []\n",
    "X_vals = []\n",
    "y_vals = []\n",
    "for i in range(1, 3 + 1):\n",
    "    train_portion = np.round(i * VAL_PORTION, 2)\n",
    "    div_0 = new_quarters[0]\n",
    "    div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "    div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "    X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "    y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e336604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa01891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed06e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f42b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d50042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load(\"../results/Ridge(19:57 on 2-12-2023).pkl\")\n",
    "cfs = pd.DataFrame(model.named_steps[\"model\"].coef_)\n",
    "cfs.index = model.named_steps['preprocess'].get_feature_names_out()\n",
    "cfs[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7f5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load(\"../results/Ridge(22:59 on 3-12-2023).pkl\")\n",
    "cfs = pd.DataFrame(model.named_steps[\"model\"].coef_)\n",
    "cfs.index = model.named_steps['preprocess'].get_feature_names_out()\n",
    "cfs[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53aee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old split checking\n",
    "def check_split_sizes(X, train, test, val):\n",
    "    fails = 0\n",
    "    print(\"[*] checking train test val split\")\n",
    "    train_set_qs = set(train[\"TIME_PERIOD\"])\n",
    "    test_set_qs = set(test[\"TIME_PERIOD\"])\n",
    "    val_set_qs = set(val[\"TIME_PERIOD\"])\n",
    "    \n",
    "    #check for TIME_PERIOD overlap\n",
    "    shared = (train_set_qs & test_set_qs) | (val_set_qs & test_set_qs) | (train_set_qs & val_set_qs)\n",
    "    if (len(shared) != 0):\n",
    "        warnings.warn('\\t[-] overlap between train, test, or val time_periods')\n",
    "        fails+=1\n",
    "    else:\n",
    "        print(\"\\t[+] no overlap between train, test, or val TIME_PERIODS\")\n",
    "        \n",
    "    #check for a fairly even 60/20/20 split\n",
    "    NAMES = ['train', 'test ', 'val  ']\n",
    "    TARGETS = [0.6, 0.2, 0.2]\n",
    "    ALLOWED_FRACTION_ERROR = 0.02\n",
    "    sizes = [len(train) / len(X), len(test) / len(X), len(val) / len(X)]\n",
    "    for i in range(0, 3):\n",
    "        if (np.abs(sizes[i] - TARGETS[i]) < ALLOWED_FRACTION_ERROR):\n",
    "            print(\"\\t[+] \" + str(NAMES[i]) + \" is \" + str(np.round(sizes[i], 3)) + \" of datapoints which is within bounds of its \" + str(TARGETS[i]) + \" target\")\n",
    "        else:\n",
    "            warnings.warn(\"\\t[-] \" + str(NAMES[i]) + \" is \" + str(np.round(sizes[i], 3)) + \" of datapoints which is out of bounds\")\n",
    "            fails+=1\n",
    "\n",
    "    if (fails == 0):\n",
    "        print(\"\\t[+] \\x1b[42mPASSED ALL\\x1b[0m train test val split tests\")\n",
    "    else:\n",
    "        print(\"\\t[?] \\033[91mFAILED \" + str(fails) + \"\\033[0m train test val split tests\")\n",
    "        \n",
    "\n",
    "#check_split_sizes(X, X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d982fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    X_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(1, 3 + 1):\n",
    "        train_portion = np.round(i * VAL_PORTION, 2)\n",
    "        div_0 = new_quarters[0]\n",
    "        div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "        div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "        X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate baselines\n",
    "y_pred1 = np.zeros(len(y_test))\n",
    "base_zeros = RMSE(y_pred1, y_test)\n",
    "\n",
    "test_mean = y_trains[2].mean()\n",
    "y_pred2 = [test_mean] * len(y_test)\n",
    "base_test_mean = RMSE(y_pred2, y_test)\n",
    "\n",
    "y_pred3 = X_test[\"TOTAL_POS - lag 1 quarter\"]\n",
    "base_last = RMSE(y_pred3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "model_ridge = joblib.load(\"../results/Ridge(19:57 on 2-12-2023).pkl\")\n",
    "model_ridge = joblib.load(\"../results/Ridge(20:30 on 2-12-2023).pkl\") #this is the better updated ridge model\n",
    "y_pred_ridge = model_ridge.predict(X_test)\n",
    "score_ridge = RMSE(y_pred_ridge, y_test)\n",
    "\n",
    "model_poly = joblib.load(\"../results/Ridge(22:58 on 3-12-2023).pkl\") #this is the better updated ridge model\n",
    "y_pred_poly = model_poly.predict(X_test)\n",
    "score_poly = RMSE(y_pred_poly, y_test)\n",
    "\n",
    "model_SVR = joblib.load(\"../results/LinearSVR(16:40 on 3-12-2023).pkl\")\n",
    "y_pred_SVR = model_SVR.predict(X_test)\n",
    "score_SVR = RMSE(y_pred_SVR, y_test)\n",
    "\n",
    "model_RF = joblib.load(\"../results/RandomForestRegressor(22:46 on 2-12-2023).pkl\")\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "score_RF = RMSE(y_pred_RF, y_test)\n",
    "\n",
    "model_XGB = joblib.load(\"../results/XGBRegressor(0:14 on 4-12-2023).pkl\")\n",
    "y_pred_XGB = model_XGB.predict(preprocessor.transform(X_test))\n",
    "score_XGB = RMSE(y_pred_XGB, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = joblib.load(\"../results/RandomForestRegressor(18:43 on 4-12-2023).pkl\")\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "score_RF = RMSE(y_pred_RF, y_test)\n",
    "score_RF\n",
    "#RandomForestRegressor(18:41 on 4-12-2023).pkl\n",
    "#RF 14 (1:01 on 3): 7.478723543421034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25946758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_score_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ff26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(y_pred_ridge, columns=['y_pred'])\n",
    "preds.index = y_test.index\n",
    "preds['y_pred_other'] = new_y_pred_ridge\n",
    "preds['y_true'] = y_test\n",
    "preds.sort_values(by='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = cfs_from(new_model_ridge)\n",
    "cfs[cfs.index.list.find(\"poly\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs.filter(like='poly', axis=0)\n",
    "cfs[cfs['importance'] > 0.000114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db16cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs_from(model_poly).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cfs_from(model):\n",
    "    cfs = pd.DataFrame(model.named_steps[\"model\"].coef_, columns=['importance'])\n",
    "    cfs.index = model.named_steps['preprocess'].get_feature_names_out()\n",
    "    cfs['abs_importance'] = cfs['importance'].abs()\n",
    "    return cfs.sort_values(by='abs_importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.named_steps['preprocess'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d532059",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650c131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************feature scaling********************************************************\n",
    "\n",
    "ordinal_ftrs = ['age', 'TIME_PERIOD']\n",
    "ordinal_cats = [['UNK','Y_LT14','Y14-17','Y18-34','Y35-64','Y_GE65'], quarters]\n",
    "                                                                     #^^i'm using quarters not new_quarters here so that\n",
    "                                                                     #  the model can still tell where in history this q is\n",
    "onehot_ftrs = ['citizen', 'geo', 'sex']\n",
    "#onehot_ftrs = ['geo', 'sex']\n",
    "minmax_ftrs = []\n",
    "std_ftrs = [a for a in X.columns.to_list() if 'TOTAL' in a]\n",
    "poly_ftrs = ['TOTAL_APPS', 'TOTAL_POS - lag 1 quarter', \"TOTAL_APPS - lag 1 quarter\"]\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "preprocessor_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs),\n",
    "        ('poly', PolynomialFeatures(degree=(2,2), interaction_only=False), poly_ftrs)\n",
    "    ])\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "#X_train_prep = clf.fit_transform(X_train)\n",
    "#X_val_prep = clf.transform(X_val)\n",
    "#X_test_prep = clf.transform(X_test)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train_prep.shape)\n",
    "#print(X_train_prep)\n",
    "#X_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55328c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1c98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab34edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XGB = model_XGB\n",
    "zeros = np.zeros(len(XGB._Booster.get_score(fmap='', importance_type='total_cover')))\n",
    "\n",
    "metrics = pd.DataFrame(zeros, columns=[\"0\"])\n",
    "\n",
    "metric_list = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']\n",
    "\n",
    "for m in metric_list:\n",
    "    metrics[m] = list(XGB._Booster.get_score(fmap='', importance_type=m).values())\n",
    "\n",
    "metrics = metrics.drop(\"0\", axis=1)\n",
    "metrics.index = list(XGB._Booster.get_score(fmap='', importance_type=m).keys())\n",
    "metrics.index = list(feature_names[[int(e[1:]) for e in metrics.index]])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc82f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542d218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227223b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcf456",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_TO_SHOW = 10\n",
    "for m in metric_list:\n",
    "    sorted = metrics.sort_values(by=m, ascending=False)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(sorted.index[:NUM_TO_SHOW][::-1],sorted[m][:NUM_TO_SHOW][::-1])\n",
    "    plt.xlabel(m)\n",
    "    plt.ylabel('feature')\n",
    "    plt.title('top ' + str(NUM_TO_SHOW) + \" features with most \" + m)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(XGB)\n",
    "shap_values = explainer.shap_values(preprocessor.transform(X_test))\n",
    "\n",
    "shap.summary_plot(shap_values, feature_names=df_test.columns, max_display=10, title='Shap Summary Plot', plot_type='bar', show=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb1da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
