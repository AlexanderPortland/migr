{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa6b5c",
   "metadata": {},
   "source": [
    "# Splitting, Preprocessing and Model Development\n",
    "This notebook is used for:\n",
    "\n",
    "\n",
    "### Declaring Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318f1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import sklearn\n",
    "import time\n",
    "import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a4beb",
   "metadata": {},
   "source": [
    "### Color Palette & Typeface Sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9835e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "YELLOW = '#F2DC5D'\n",
    "GREEN = '#9BC53D'\n",
    "DARK_GREEN = '#597222'\n",
    "RED = '#C3423F'\n",
    "LIGHT_BLUE = '#2596BE'\n",
    "GRAY = '#666666'\n",
    "\n",
    "AXIS_SIZE = 12\n",
    "TITLE_SIZE = 16\n",
    "DESCRIPTION_SIZE = 9\n",
    "FIGURE_SIZE = (10*2/3,6*2/3)\n",
    "\n",
    "RANDOM_STATE = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849d3c5",
   "metadata": {},
   "source": [
    "### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f2f082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citizen</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>geo</th>\n",
       "      <th>TIME_PERIOD</th>\n",
       "      <th>GENCONV</th>\n",
       "      <th>HUMSTAT</th>\n",
       "      <th>SUB_PROT</th>\n",
       "      <th>REJECTED</th>\n",
       "      <th>TOTAL_POS</th>\n",
       "      <th>TOTAL_APPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2008-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221109</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221110</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221111</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221112</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221113</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7213806 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citizen  sex     age geo TIME_PERIOD  GENCONV  HUMSTAT  SUB_PROT  \\\n",
       "0            AD    F     UNK  AT     2008-Q1        0        0         0   \n",
       "1            AD    F     UNK  AT     2008-Q2        0        0         0   \n",
       "2            AD    F     UNK  AT     2008-Q3        0        0         0   \n",
       "3            AD    F     UNK  AT     2008-Q4        0        0         0   \n",
       "4            AD    F     UNK  AT     2009-Q1        0        0         0   \n",
       "...         ...  ...     ...  ..         ...      ...      ...       ...   \n",
       "7221109      ZW  UNK  Y_LT14  UK     2019-Q3        0        0         0   \n",
       "7221110      ZW  UNK  Y_LT14  UK     2019-Q4        0        0         0   \n",
       "7221111      ZW  UNK  Y_LT14  UK     2020-Q1        0        0         0   \n",
       "7221112      ZW  UNK  Y_LT14  UK     2020-Q2        0        0         0   \n",
       "7221113      ZW  UNK  Y_LT14  UK     2020-Q3        0        0         0   \n",
       "\n",
       "         REJECTED  TOTAL_POS  TOTAL_APPS  \n",
       "0               0          0           0  \n",
       "1               0          0           0  \n",
       "2               0          0           0  \n",
       "3               0          0           0  \n",
       "4               0          0           0  \n",
       "...           ...        ...         ...  \n",
       "7221109         0          0           0  \n",
       "7221110         0          0           0  \n",
       "7221111         0          0           0  \n",
       "7221112         0          0           0  \n",
       "7221113         0          0           0  \n",
       "\n",
       "[7213806 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#****************************************************import dataset****************************************************\n",
    "df = pd.read_csv('../data/final.csv', dtype={'citizen': 'string', 'sex': 'string', 'age': 'string', 'decision': 'string', 'geo': 'string', 'TIME_PERIOD': 'string', 'GENCONV': \"Int64\", 'HUMSTAT': \"Int64\", 'SUB_PROT': \"Int64\", 'REJECTED': \"Int64\", 'TOTAL_POS': \"Int64\", 'TOTAL_APPS': \"Int64\", \"POS_RATE\": \"Float64\"}, keep_default_na=False, na_values=['nan'])\n",
    "\n",
    "##remove partial 2023-Q3 Data\n",
    "df = df[df[\"TIME_PERIOD\"] != \"2023-Q3\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500d090",
   "metadata": {},
   "source": [
    "## 1. Introduce Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5da77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citizen</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>geo</th>\n",
       "      <th>TIME_PERIOD</th>\n",
       "      <th>GENCONV</th>\n",
       "      <th>HUMSTAT</th>\n",
       "      <th>SUB_PROT</th>\n",
       "      <th>REJECTED</th>\n",
       "      <th>TOTAL_POS</th>\n",
       "      <th>TOTAL_APPS</th>\n",
       "      <th>TOTAL_POS - lag 1 quarter</th>\n",
       "      <th>TOTAL_POS - lag 2 quarters</th>\n",
       "      <th>TOTAL_POS - lag 3 quarters</th>\n",
       "      <th>TOTAL_POS - lag 4 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 1 quarter</th>\n",
       "      <th>TOTAL_APPS - lag 2 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 3 quarters</th>\n",
       "      <th>TOTAL_APPS - lag 4 quarters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2009-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AD</td>\n",
       "      <td>F</td>\n",
       "      <td>UNK</td>\n",
       "      <td>AT</td>\n",
       "      <td>2010-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221109</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221110</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2019-Q4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221111</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221112</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221113</th>\n",
       "      <td>ZW</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Y_LT14</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6731478 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citizen  sex     age geo TIME_PERIOD  GENCONV  HUMSTAT  SUB_PROT  \\\n",
       "4            AD    F     UNK  AT     2009-Q1        0        0         0   \n",
       "5            AD    F     UNK  AT     2009-Q2        0        0         0   \n",
       "6            AD    F     UNK  AT     2009-Q3        0        0         0   \n",
       "7            AD    F     UNK  AT     2009-Q4        0        0         0   \n",
       "8            AD    F     UNK  AT     2010-Q1        0        0         0   \n",
       "...         ...  ...     ...  ..         ...      ...      ...       ...   \n",
       "7221109      ZW  UNK  Y_LT14  UK     2019-Q3        0        0         0   \n",
       "7221110      ZW  UNK  Y_LT14  UK     2019-Q4        0        0         0   \n",
       "7221111      ZW  UNK  Y_LT14  UK     2020-Q1        0        0         0   \n",
       "7221112      ZW  UNK  Y_LT14  UK     2020-Q2        0        0         0   \n",
       "7221113      ZW  UNK  Y_LT14  UK     2020-Q3        0        0         0   \n",
       "\n",
       "         REJECTED  TOTAL_POS  TOTAL_APPS  TOTAL_POS - lag 1 quarter  \\\n",
       "4               0          0           0                          0   \n",
       "5               0          0           0                          0   \n",
       "6               0          0           0                          0   \n",
       "7               0          0           0                          0   \n",
       "8               0          0           0                          0   \n",
       "...           ...        ...         ...                        ...   \n",
       "7221109         0          0           0                          0   \n",
       "7221110         0          0           0                          0   \n",
       "7221111         0          0           0                          0   \n",
       "7221112         0          0           0                          0   \n",
       "7221113         0          0           0                          0   \n",
       "\n",
       "         TOTAL_POS - lag 2 quarters  TOTAL_POS - lag 3 quarters  \\\n",
       "4                                 0                           0   \n",
       "5                                 0                           0   \n",
       "6                                 0                           0   \n",
       "7                                 0                           0   \n",
       "8                                 0                           0   \n",
       "...                             ...                         ...   \n",
       "7221109                           0                           0   \n",
       "7221110                           0                           0   \n",
       "7221111                           0                           0   \n",
       "7221112                           0                           0   \n",
       "7221113                           0                           0   \n",
       "\n",
       "         TOTAL_POS - lag 4 quarters  TOTAL_APPS - lag 1 quarter  \\\n",
       "4                                 0                           0   \n",
       "5                                 0                           0   \n",
       "6                                 0                           0   \n",
       "7                                 0                           0   \n",
       "8                                 0                           0   \n",
       "...                             ...                         ...   \n",
       "7221109                           0                           0   \n",
       "7221110                           0                           0   \n",
       "7221111                           0                           0   \n",
       "7221112                           0                           0   \n",
       "7221113                           0                           0   \n",
       "\n",
       "         TOTAL_APPS - lag 2 quarters  TOTAL_APPS - lag 3 quarters  \\\n",
       "4                                  0                            0   \n",
       "5                                  0                            0   \n",
       "6                                  0                            0   \n",
       "7                                  0                            0   \n",
       "8                                  0                            0   \n",
       "...                              ...                          ...   \n",
       "7221109                            0                            0   \n",
       "7221110                            0                            0   \n",
       "7221111                            0                            0   \n",
       "7221112                            0                            0   \n",
       "7221113                            0                            0   \n",
       "\n",
       "         TOTAL_APPS - lag 4 quarters  \n",
       "4                                  0  \n",
       "5                                  0  \n",
       "6                                  0  \n",
       "7                                  0  \n",
       "8                                  0  \n",
       "...                              ...  \n",
       "7221109                            0  \n",
       "7221110                            0  \n",
       "7221111                            0  \n",
       "7221112                            0  \n",
       "7221113                            0  \n",
       "\n",
       "[6731478 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#****************************************************re-sort dataframe****************************************************\n",
    "\n",
    "sort_order = ['citizen', 'sex', 'age', 'geo', 'TIME_PERIOD']\n",
    "df = df.sort_values(by =sort_order) \n",
    "\n",
    "#*********************************************create sequential list of quarters****************************************************\n",
    "\n",
    "quarters = []\n",
    "for i in range(2008, 2024):\n",
    "    quarters.append(str(i) + \"-Q1\")\n",
    "    quarters.append(str(i) + \"-Q2\")\n",
    "    quarters.append(str(i) + \"-Q3\")\n",
    "    quarters.append(str(i) + \"-Q4\")\n",
    "\n",
    "#****************************************************lagged features****************************************************\n",
    "\n",
    "QUARTERS_OF_LAG = (4 * 1)\n",
    "\n",
    "def add_lagged_features(df, features, maintained_columns, QUARTERS_OF_LAG):\n",
    "    quarters = np.unique(df[\"TIME_PERIOD\"])\n",
    "    def lagged_features(target_var, lag_count, unit):\n",
    "        lagged = pd.DataFrame()\n",
    "        columns = []\n",
    "        for i in range(1, lag_count + 1):\n",
    "            lagged = pd.concat([lagged, target_var.shift(i)], axis=1)\n",
    "            name = target_var.name\n",
    "            if (i == 1):\n",
    "                columns.append(name + \" - lag \" + str(i) + \" \" + str(unit))\n",
    "            else:\n",
    "                columns.append(name + \" - lag \" + str(i) + \" \" + str(unit) + \"s\")\n",
    "        lagged.columns = columns\n",
    "        return lagged.astype('Int64')\n",
    "\n",
    "    #introduce lag for each feature\n",
    "    df_lagged = df\n",
    "    for f in features:\n",
    "        df_lagged = pd.concat([df_lagged, lagged_features(df[f], QUARTERS_OF_LAG, \"quarter\")], axis=1)\n",
    "\n",
    "    #remove all features with less than the lag amount of historical data\n",
    "    #df_lagged = df_lagged[df_lagged.eq()]\n",
    "    for i in range(0, QUARTERS_OF_LAG):\n",
    "        shift_eq = df_lagged.eq(df_lagged.shift())\n",
    "        keep = shift_eq[maintained_columns[0]]\n",
    "        for j in range(1, len(maintained_columns)):\n",
    "            keep = keep & shift_eq[maintained_columns[j]]\n",
    "        df_lagged = df_lagged[keep]\n",
    "        #print(\"lagged i\" + str(i) + \" of \" + str(QUARTERS_OF_LAG))\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "df_lagged = add_lagged_features(df, [\"TOTAL_POS\", \"TOTAL_APPS\"], ['citizen', 'age', 'sex', 'geo'], QUARTERS_OF_LAG)\n",
    "\n",
    "df_lagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e5062",
   "metadata": {},
   "source": [
    "### Lagged Feature Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45cfc60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] testing lagging function\n",
      "\t[+] basic lag tests passed\n",
      "\t[+] tests with shared times passed\n",
      "\t[+] tests with multiple start and end dates passed\n",
      "\t[+] \u001b[42mPASSED ALL\u001b[0m lagging tests\n"
     ]
    }
   ],
   "source": [
    "#***********************************************testing equivalence function****************************************************\n",
    "def equivalent_dfs(df1, df2):\n",
    "    return (df1.reset_index(drop=True) == df2.reset_index(drop=True)).all().all()\n",
    "\n",
    "#****************************************************lagging testing****************************************************\n",
    "print(\"[*] testing lagging function\")\n",
    "#simple tests\n",
    "tdf0_data = [['AR', 1, 1], ['AR', 2, 2], ['AR', 3, 3], ['AR', 4, 4], ['AR', 5, 5]]\n",
    "tdf0 = pd.DataFrame(data=tdf0_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf1_data = [['AR', 2, 2, 1], ['AR', 3, 3, 2], ['AR', 4, 4, 3], ['AR', 5, 5, 4]]\n",
    "tdf1 = pd.DataFrame(data=tdf1_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf2_data = [['AR', 3, 3, 2, 1], ['AR', 4, 4, 3, 2], ['AR', 5, 5, 4, 3]]\n",
    "tdf2 = pd.DataFrame(data=tdf2_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "tdf3_data = [['AR', 4, 4, 3, 2, 1], ['AR', 5, 5, 4, 3, 2]]\n",
    "tdf3 = pd.DataFrame(data=tdf3_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters', 'val - lag 3 quarters'])\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 1), \n",
    "                      tdf1), \"basic lag test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 2), \n",
    "                      tdf2), \"basic lag test failed, shift=2\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf0, [\"val\"], ['citizen'], 3), \n",
    "                      tdf3), \"basic lag test failed, shift=3\"\n",
    "print(\"\\t[+] basic lag tests passed\")\n",
    "\n",
    "#more advanced tests\n",
    "tdf4_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "             ['NZ', 1, 11], ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15]]\n",
    "tdf4 = pd.DataFrame(data=tdf4_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf5_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "             ['NZ', 2, 12, 11], ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14]]\n",
    "tdf5 = pd.DataFrame(data=tdf5_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf6_data = [['AU', 3, 3, 2, 1], ['AU', 4, 4, 3, 2], ['AU', 5, 5, 4, 3],\n",
    "             ['NZ', 3, 13, 12, 11], ['NZ', 4, 14, 13, 12], ['NZ', 5, 15, 14, 13]]\n",
    "tdf6 = pd.DataFrame(data=tdf6_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf4, [\"val\"], ['citizen'], 1), \n",
    "                      tdf5), \"lag with shared times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf4, [\"val\"], ['citizen'], 2), \n",
    "                      tdf6), \"lag with shared times test failed, shift=2\"\n",
    "print(\"\\t[+] tests with shared times passed\")\n",
    "\n",
    "#very complicated tests\n",
    "tdf7_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "             ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15]]\n",
    "tdf7 = pd.DataFrame(data=tdf7_data, columns=['citizen', 'TIME_PERIOD', 'val'])\n",
    "tdf8_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "             ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14]]\n",
    "tdf8 = pd.DataFrame(data=tdf8_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter'])\n",
    "tdf9_data = [['AU', 3, 3, 2, 1], ['AU', 4, 4, 3, 2], ['AU', 5, 5, 4, 3],\n",
    "             ['NZ', 4, 14, 13, 12], ['NZ', 5, 15, 14, 13]]\n",
    "tdf9 = pd.DataFrame(data=tdf9_data, columns=['citizen', 'TIME_PERIOD', 'val', 'val - lag 1 quarter', 'val - lag 2 quarters'])\n",
    "tdf10_data = [['AU', 1, 1], ['AU', 2, 2], ['AU', 3, 3], ['AU', 4, 4], ['AU', 5, 5],\n",
    "              ['NZ', 2, 12], ['NZ', 3, 13], ['NZ', 4, 14], ['NZ', 5, 15],\n",
    "              ['FJ', 3, 23], ['FJ', 4, 24], ['FJ', 5, 25], ['FJ', 6, 26], ['FJ', 7, 27], ['FJ', 8, 28],\n",
    "              ['WS', 4, 34], ['WS', 5, 35]]\n",
    "tdf10 = pd.DataFrame(data=tdf10_data, columns=['citizen', 'TIME_PERIOD', 'new_val'])\n",
    "tdf11_data = [['AU', 2, 2, 1], ['AU', 3, 3, 2], ['AU', 4, 4, 3], ['AU', 5, 5, 4],\n",
    "              ['NZ', 3, 13, 12], ['NZ', 4, 14, 13], ['NZ', 5, 15, 14],\n",
    "              ['FJ', 4, 24, 23], ['FJ', 5, 25, 24], ['FJ', 6, 26, 25], ['FJ', 7, 27, 26], ['FJ', 8, 28, 27],\n",
    "              ['WS', 5, 35, 34]]\n",
    "tdf11 = pd.DataFrame(data=tdf11_data, columns=['citizen', 'TIME_PERIOD', 'new_val', 'new_val - lag 1 quarter'])\n",
    "tdf12_data = [['AU', 5, 5, 4, 3, 2, 1],\n",
    "              ['FJ', 7, 27, 26, 25, 24, 23], ['FJ', 8, 28, 27, 26, 25, 24]]\n",
    "tdf12 = pd.DataFrame(data=tdf12_data, columns=['citizen', 'TIME_PERIOD', 'new_val', 'new_val - lag 1 quarter', 'new_val - lag 2 quarters', 'new_val - lag 3 quarters', 'new_val - lag 4 quarters'])\n",
    "\n",
    "assert equivalent_dfs(add_lagged_features(tdf7, [\"val\"], ['citizen'], 1), \n",
    "                      tdf8), \"lag with 2 different start times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf7, [\"val\"], ['citizen'], 2), \n",
    "                      tdf9), \"lag with 2 different start times test failed, shift=2\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf10, [\"new_val\"], ['citizen'], 1), \n",
    "                      tdf11), \"lag with many different start times test failed, shift=1\"\n",
    "assert equivalent_dfs(add_lagged_features(tdf10, [\"new_val\"], ['citizen'], 4), \n",
    "                      tdf12), \"lag with many different start times test failed, shift=4\"\n",
    "print(\"\\t[+] tests with multiple start and end dates passed\")\n",
    "\n",
    "print(\"\\t[+] \\x1b[42mPASSED ALL\\x1b[0m lagging tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd114121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d6700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb22ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1acfa54",
   "metadata": {},
   "source": [
    "## 2. Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ace99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************splitting****************************************************\n",
    "TARGET_VAR = \"TOTAL_POS\"\n",
    "\n",
    "y = df_lagged[TARGET_VAR]\n",
    "X = df_lagged.drop(['GENCONV', 'HUMSTAT', 'SUB_PROT', 'REJECTED', 'TOTAL_POS'], axis=1)\n",
    "#PLAN:\n",
    "#of 62 quarters...\n",
    "#QUARTERS_OF_LAG are lost bc they wont have the needed lagged features\n",
    "    \n",
    "new_quarters = [q for q in quarters if q >= quarters[QUARTERS_OF_LAG]]\n",
    "quarter_count = len(new_quarters) - 1\n",
    "\n",
    "TRAIN_PORTION = 0.6\n",
    "VAL_PORTION = 0.2\n",
    "TEST_PORTION = 0.2\n",
    "\n",
    "#take out last portion of quarters for testing\n",
    "#div_0 = new_quarters[0]\n",
    "#div_1 = new_quarters[int(quarter_count * train_split)]\n",
    "div_2 = new_quarters[int(quarter_count * (1 - TEST_PORTION))]\n",
    "div_3 = new_quarters[quarter_count]\n",
    "\n",
    "#seperate out test section\n",
    "X_test = X[(div_2 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_3)]\n",
    "y_test = y[(div_2 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd81e4b",
   "metadata": {},
   "source": [
    "### Evaluation Functions (for cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f29819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).sum() / len(y_pred))\n",
    "\n",
    "def BASELINE_RMSE(y_true):\n",
    "    return ALL_ZERO_BASELINE(y_true)\n",
    "\n",
    "def ALL_ZERO_BASELINE(y_true):\n",
    "    return np.sqrt(((y_true) ** 2).sum() / len(y_true))\n",
    "\n",
    "def ALL_ONE_BASELINE(y_true):\n",
    "    return np.sqrt(((1 - y_true) ** 2).sum() / len(y_true))\n",
    "\n",
    "def ALL_MEAN_BASELINE(y_train, y_true):\n",
    "    mean = y_train.mean()\n",
    "    return np.sqrt(((mean - y_true) ** 2).sum() / len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a4a15",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d25fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIMESTAMP_STR():\n",
    "    dt = datetime.datetime.now()\n",
    "    txt = \"{hour}:{minute:02.0f} on {day}-{month}-{year}\"\n",
    "    return txt.format(hour=dt.hour, minute=dt.minute, day=dt.day, month=dt.month, year=dt.year)\n",
    "\n",
    "def DF_ALL_PREDICTED(model):\n",
    "    df_new = df\n",
    "    df_new[\"TOTAL_APPS_PRED\"] = model.predict(X)\n",
    "\n",
    "UPDATE = True\n",
    "def Status_Update(t, message):\n",
    "    my_time = time.time()\n",
    "    if UPDATE:\n",
    "        print(str(message) + \" \\t-- in \" + str(time.time() - t) + \"s\")\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f840ef3",
   "metadata": {},
   "source": [
    "### General Model-Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5278a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPipe_TimeSeries_RMSE(X, y, preprocessor, ML_algo, param_grid):\n",
    "    ALGO_NAME = str(ML_algo)[str(ML_algo).rindex('.')+1:-2]\n",
    "    MODEL_NAME = \"{algo}({time})\".format(algo=ALGO_NAME, time=TIMESTAMP_STR())\n",
    "    NUM_FOLDS = np.round(TRAIN_PORTION / VAL_PORTION).astype(int)\n",
    "    RANDOM_STATE = 14\n",
    "    print(\"[!] looking at model: \" + str(MODEL_NAME))\n",
    "    \n",
    "    print(\"\\t[*] doing initial dataset splitting\")\n",
    "    \n",
    "    #opt check 2\n",
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    X_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(1, NUM_FOLDS + 1):\n",
    "        train_portion = np.round(i * VAL_PORTION, 2)\n",
    "        div_0 = new_quarters[0]\n",
    "        div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "        div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "        X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        print(\"\\t\\t[+] fold \" + str(i))\n",
    "    \n",
    "    param_scores = []\n",
    "    param_models = []\n",
    "    for p in param_grid:\n",
    "        T_PARAM_START = time.time()\n",
    "        fold_scores = []\n",
    "        print(\"\\t[*] looking at hyperparameters \" + str(p))\n",
    "        for i in range(1, NUM_FOLDS + 1):\n",
    "            T_FOLD_START = time.time()\n",
    "            #ADD OPTIMIZATION HERE:\n",
    "                #[2] NO NEED TO CONSTANTLY REDIVIDE THE DATA, SAME THREE FOLDS WILL BE USED EACH TIME REGARDLES OF HYPERPARAMAETERS\n",
    "                #[1] NO NEED TO CHECK FUTURE FOLDS IF EVEN A SCORE OF ZERO WOULDN'T BRING IT BELOW A BETTER FOUND PARAMETER ARRANGEMENT\n",
    "            train_portion = np.round(i * VAL_PORTION, 2)\n",
    "            \n",
    "            #opt check 1\n",
    "            if (i > 0 & len(param_scores) > 0):\n",
    "                best_param_score = param_scores[np.argmin(param_scores)]\n",
    "                if ((best_param_score * 3) < np.sum(fold_scores)):\n",
    "                    print(\"[!] best average score I could get with \" + str(np.sum(fold_scores)) + \" score already is worse than a previous params score of \" + str(best_param_score) + \", so giving up\")\n",
    "\n",
    "            mini_t = time.time()\n",
    "            \n",
    "            X_train = X_trains[i - 1]\n",
    "            y_train = y_trains[i - 1]\n",
    "            X_val = X_vals[i - 1]\n",
    "            y_val = y_vals[i - 1]\n",
    "            \n",
    "            #PORTION_OF_POINTS = 0.001\n",
    "            #X_train = X_train.sample(np.floor((len(X_train) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            #y_train = y_train.loc[X_train.index]\n",
    "            #X_val = X_val.sample(np.floor((len(X_val) * PORTION_OF_POINTS)).astype(int), random_state = RANDOM_STATE)\n",
    "            #y_val = y_val.loc[X_val.index]\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"split data\")\n",
    "\n",
    "            #make pipeline\n",
    "            algo = ML_algo(**p, random_state = RANDOM_STATE)\n",
    "            \n",
    "            #print(algo)\n",
    "            pipe = Pipeline(steps=[\n",
    "                        ('preprocess', preprocessor),\n",
    "                        ('model', algo)\n",
    "                    ], verbose=True)\n",
    "            \n",
    "            mini_t = Status_Update(mini_t, \"made pipeline\")\n",
    "            #print(X_train)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            mini_t = Status_Update(mini_t, \"fit pipeline\")\n",
    "            \n",
    "            y_pred = pipe.predict(X_val)\n",
    "            score = RMSE(y_pred, y_val)\n",
    "            \n",
    "            fold_scores.append(score)\n",
    "            T_FOLD_END = time.time()\n",
    "            T_FOLD_ELAPSED = T_FOLD_END - T_FOLD_START\n",
    "            mini_t = Status_Update(mini_t, \"eval pipeline\")\n",
    "            print(\"\\t\\t[*] fold \" + str(i) + \" complete (test score of \" + str(score) + \") -- in \" + str(np.round(T_FOLD_ELAPSED, 3)) + \"s\")\n",
    "        score = np.mean(fold_scores)\n",
    "        param_scores.append(score)\n",
    "        param_models.append(pipe)\n",
    "        \n",
    "        T_PARAM_END = time.time()\n",
    "        T_PARAM_ELAPSED = T_PARAM_END - T_PARAM_START\n",
    "        print(\"\\t\\t[+] final score for params of \" + str(score) + \" -- in \" + str(np.round(T_PARAM_ELAPSED, 3)) + \"s\")\n",
    "        \n",
    "        if (np.argmin(param_scores) == (len(param_scores) - 1)):\n",
    "            best_model = param_models[np.argmin(param_scores)]\n",
    "            print(\"\\t\\t[!] new best param configuration, so saving model to path\")\n",
    "            path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "            joblib.dump(best_model, path, compress = 1)\n",
    "            print(\"\\t\\t\\t[+] saved model to \" + str(path))\n",
    "        \n",
    "        \n",
    "    i_best = np.argmin(param_scores)\n",
    "    best_score = param_scores[i_best]\n",
    "    best_params = param_grid[i_best]\n",
    "    best_model = param_models[i_best]\n",
    "    print(\"\\t[+] best param configuration of \" + str(best_params) + \" found with score \" + str(best_score))\n",
    "    path = \"../results/\" + MODEL_NAME + \".pkl\"\n",
    "    joblib.dump(best_model, path, compress = 1)\n",
    "    print(\"\\t[+] saved model to \" + str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f0d0e",
   "metadata": {},
   "source": [
    "### Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d0df76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#MODELS TO TEST:\n",
    "#[IP] LASSO\n",
    "#[T] RIDGE\n",
    "#[Q] ELASTIC NET\n",
    "#[IP] RF\n",
    "#[] SVR\n",
    "#[] XGBoost\n",
    "#[] KNN??\n",
    "\n",
    "#T: Trained\n",
    "#IP: In Progress\n",
    "#Q: Queued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c126f",
   "metadata": {},
   "source": [
    "### Training Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6340e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LASSO_PARAM_GRID = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "\n",
    "ELASTIC_PARAM_GRID = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.1, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            'l1_ratio': np.linspace(0.2, 1, 9)\n",
    "            })\n",
    "\n",
    "#MLPipe_TimeSeries_RMSE(X, y, preprocessor, ElasticNet, ELASTIC_PARAM_GRID)\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, Lasso, LASSO_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = []\n",
    "y_trains = []\n",
    "X_vals = []\n",
    "y_vals = []\n",
    "for i in range(1, 3 + 1):\n",
    "    train_portion = np.round(i * VAL_PORTION, 2)\n",
    "    div_0 = new_quarters[0]\n",
    "    div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "    div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "    X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "    y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca6cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    X_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(1, 3 + 1):\n",
    "        train_portion = np.round(i * VAL_PORTION, 2)\n",
    "        div_0 = new_quarters[0]\n",
    "        div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "        div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "        X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "        X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "        y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52c9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f6df889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] looking at model: Ridge(22:38 on 2-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'alpha': 0.01}\n",
      "split data \t-- in 8.106231689453125e-06s\n",
      "made pipeline \t-- in 8.988380432128906e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   2.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   1.9s\n",
      "fit pipeline \t-- in 4.138293981552124s\n",
      "eval pipeline \t-- in 1.9484481811523438s\n",
      "\t\t[*] fold 1 complete (test score of 4.03857183989241) -- in 6.087s\n",
      "split data \t-- in 6.198883056640625e-06s\n",
      "made pipeline \t-- in 3.6716461181640625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   6.2s\n",
      "fit pipeline \t-- in 11.53640079498291s\n",
      "eval pipeline \t-- in 1.8799757957458496s\n",
      "\t\t[*] fold 2 complete (test score of 19.97768988872798) -- in 13.416s\n",
      "split data \t-- in 1.1920928955078125e-05s\n",
      "made pipeline \t-- in 3.814697265625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=  21.3s\n",
      "fit pipeline \t-- in 29.960400819778442s\n",
      "eval pipeline \t-- in 1.812638282775879s\n",
      "\t\t[*] fold 3 complete (test score of 8.96754627458243) -- in 31.773s\n",
      "\t\t[+] final score for params of 10.994602667734272 -- in 51.277s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/Ridge(22:38 on 2-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'alpha': 0.03511191734215131}\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 2.002716064453125e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   2.0s\n",
      "fit pipeline \t-- in 3.263061046600342s\n",
      "eval pipeline \t-- in 2.018141031265259s\n",
      "\t\t[*] fold 1 complete (test score of 4.0385718510471715) -- in 5.281s\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 2.7179718017578125e-05s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m RIDGE_PARAM_GRID \u001b[38;5;241m=\u001b[39m ParameterGrid({\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mgeomspace(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m12\u001b[39m), \u001b[38;5;66;03m# no upper bound so the values are evenly spaced in log\u001b[39;00m\n\u001b[1;32m      3\u001b[0m             })\n\u001b[1;32m      4\u001b[0m RIDGE_PARAM_GRID_SMALL \u001b[38;5;241m=\u001b[39m ParameterGrid({\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m], \u001b[38;5;66;03m# no upper bound so the values are evenly spaced in log\u001b[39;00m\n\u001b[1;32m      6\u001b[0m             })\n\u001b[0;32m----> 8\u001b[0m MLPipe_TimeSeries_RMSE(X, y, preprocessor, Ridge, RIDGE_PARAM_GRID)\n",
      "Cell \u001b[0;32mIn[28], line 72\u001b[0m, in \u001b[0;36mMLPipe_TimeSeries_RMSE\u001b[0;34m(X, y, preprocessor, ML_algo, param_grid)\u001b[0m\n\u001b[1;32m     70\u001b[0m mini_t \u001b[38;5;241m=\u001b[39m Status_Update(mini_t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmade pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#print(X_train)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     73\u001b[0m mini_t \u001b[38;5;241m=\u001b[39m Status_Update(mini_t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    371\u001b[0m     cloned_transformer,\n\u001b[1;32m    372\u001b[0m     X,\n\u001b[1;32m    373\u001b[0m     y,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    376\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    664\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    666\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    668\u001b[0m )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    671\u001b[0m         delayed(func)(\n\u001b[1;32m    672\u001b[0m             transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m    673\u001b[0m             X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, column, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    674\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    675\u001b[0m             weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    676\u001b[0m             message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    677\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(name, idx, \u001b[38;5;28mlen\u001b[39m(transformers)),\n\u001b[1;32m    678\u001b[0m         )\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    680\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1573\u001b[0m, in \u001b[0;36mOrdinalEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m    Transform X to ordinal codes.\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;124;03m        Transformed input.\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[1;32m   1574\u001b[0m         X,\n\u001b[1;32m   1575\u001b[0m         handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[1;32m   1576\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1577\u001b[0m         ignore_category_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices,\n\u001b[1;32m   1578\u001b[0m     )\n\u001b[1;32m   1579\u001b[0m     X_trans \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cat_idx, missing_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:225\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    222\u001b[0m             Xi[\u001b[38;5;241m~\u001b[39mvalid_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# We use check_unknown=False, since _check_unknown was\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# already called above.\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     X_int[:, i] \u001b[38;5;241m=\u001b[39m _encode(Xi, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], check_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns_with_unknown:\n\u001b[1;32m    227\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    228\u001b[0m         (\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories in columns \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RIDGE_PARAM_GRID = ParameterGrid({\n",
    "            'alpha': np.geomspace(0.01, 10000, 12), # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "RIDGE_PARAM_GRID_SMALL = ParameterGrid({\n",
    "            'alpha': [0.1, 1, 10, 100], # no upper bound so the values are evenly spaced in log\n",
    "            })\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, Ridge, RIDGE_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc011ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_PARAM_GRID = {\n",
    "            'alpha': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000], # no upper bound so the values are evenly spaced in log\n",
    "            'l1_ratio': np.linspace(0.2, 1, 9)\n",
    "            }\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, ElasticNet, ELASTIC_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e816f0",
   "metadata": {},
   "source": [
    "### Training Non-Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cbe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] looking at model: RandomForestRegressor(1:01 on 3-12-2023)\n",
      "\t[*] doing initial dataset splitting\n",
      "\t\t[+] fold 1\n",
      "\t\t[+] fold 2\n",
      "\t\t[+] fold 3\n",
      "\t[*] looking at hyperparameters {'max_depth': 1, 'max_features': 0.2}\n",
      "split data \t-- in 8.106231689453125e-06s\n",
      "made pipeline \t-- in 3.3855438232421875e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   2.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=  24.7s\n",
      "fit pipeline \t-- in 26.902783155441284s\n",
      "eval pipeline \t-- in 3.203242063522339s\n",
      "\t\t[*] fold 1 complete (test score of 6.545005860560351) -- in 30.106s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 8.296966552734375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   4.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=  54.7s\n",
      "fit pipeline \t-- in 59.68383431434631s\n",
      "eval pipeline \t-- in 3.0089380741119385s\n",
      "\t\t[*] fold 2 complete (test score of 65.69335808216258) -- in 62.693s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 7.295608520507812e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.4min\n",
      "fit pipeline \t-- in 92.78087401390076s\n",
      "eval pipeline \t-- in 3.231774091720581s\n",
      "\t\t[*] fold 3 complete (test score of 17.00644957875068) -- in 96.013s\n",
      "\t\t[+] final score for params of 29.748271173824534 -- in 188.812s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 1, 'max_features': 0.4}\n",
      "split data \t-- in 1.1682510375976562e-05s\n",
      "made pipeline \t-- in 2.002716064453125e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=  46.3s\n",
      "fit pipeline \t-- in 47.46450686454773s\n",
      "eval pipeline \t-- in 3.4567129611968994s\n",
      "\t\t[*] fold 1 complete (test score of 6.3259032081454745) -- in 50.921s\n",
      "split data \t-- in 2.1457672119140625e-06s\n",
      "made pipeline \t-- in 8.225440979003906e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.7min\n",
      "fit pipeline \t-- in 108.2316620349884s\n",
      "eval pipeline \t-- in 3.2471370697021484s\n",
      "\t\t[*] fold 2 complete (test score of 65.11033863164891) -- in 111.479s\n",
      "split data \t-- in 7.867813110351562e-06s\n",
      "made pipeline \t-- in 8.511543273925781e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.7min\n",
      "fit pipeline \t-- in 167.19768619537354s\n",
      "eval pipeline \t-- in 3.5243029594421387s\n",
      "\t\t[*] fold 3 complete (test score of 18.282881947216666) -- in 170.722s\n",
      "\t\t[+] final score for params of 29.906374595670354 -- in 333.123s\n",
      "\t[*] looking at hyperparameters {'max_depth': 1, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 3.814697265625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.1min\n",
      "fit pipeline \t-- in 68.51761102676392s\n",
      "eval pipeline \t-- in 3.2514638900756836s\n",
      "\t\t[*] fold 1 complete (test score of 6.208160934139989) -- in 71.769s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 5.221366882324219e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.5min\n",
      "fit pipeline \t-- in 155.6324760913849s\n",
      "eval pipeline \t-- in 3.040985107421875s\n",
      "\t\t[*] fold 2 complete (test score of 64.94756468867351) -- in 158.674s\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 7.510185241699219e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.9min\n",
      "fit pipeline \t-- in 243.25129508972168s\n",
      "eval pipeline \t-- in 3.7535111904144287s\n",
      "\t\t[*] fold 3 complete (test score of 19.07217846440715) -- in 247.005s\n",
      "\t\t[+] final score for params of 30.075968029073547 -- in 477.448s\n",
      "\t[*] looking at hyperparameters {'max_depth': 1, 'max_features': 0.8}\n",
      "split data \t-- in 2.86102294921875e-06s\n",
      "made pipeline \t-- in 4.506111145019531e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.5min\n",
      "fit pipeline \t-- in 90.92308020591736s\n",
      "eval pipeline \t-- in 3.6184918880462646s\n",
      "\t\t[*] fold 1 complete (test score of 6.177397444734043) -- in 94.542s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 6.985664367675781e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.3min\n",
      "fit pipeline \t-- in 205.3153359889984s\n",
      "eval pipeline \t-- in 3.4394402503967285s\n",
      "\t\t[*] fold 2 complete (test score of 64.91141459101388) -- in 208.755s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 9.226799011230469e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.1min\n",
      "fit pipeline \t-- in 317.4969527721405s\n",
      "eval pipeline \t-- in 3.66953182220459s\n",
      "\t\t[*] fold 3 complete (test score of 19.79674682544749) -- in 321.167s\n",
      "\t\t[+] final score for params of 30.295186287065135 -- in 624.464s\n",
      "\t[*] looking at hyperparameters {'max_depth': 1, 'max_features': 1.0}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 3.600120544433594e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.9min\n",
      "fit pipeline \t-- in 112.90859293937683s\n",
      "eval pipeline \t-- in 3.538780927658081s\n",
      "\t\t[*] fold 1 complete (test score of 6.181222029353333) -- in 116.447s\n",
      "split data \t-- in 1.9073486328125e-06s\n",
      "made pipeline \t-- in 7.200241088867188e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.1min\n",
      "fit pipeline \t-- in 253.24681901931763s\n",
      "eval pipeline \t-- in 3.630082130432129s\n",
      "\t\t[*] fold 2 complete (test score of 64.8926606607828) -- in 256.877s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 9.822845458984375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.4min\n",
      "fit pipeline \t-- in 392.33661222457886s\n",
      "eval pipeline \t-- in 3.7331290245056152s\n",
      "\t\t[*] fold 3 complete (test score of 20.323287514814098) -- in 396.07s\n",
      "\t\t[+] final score for params of 30.465723401650077 -- in 769.395s\n",
      "\t[*] looking at hyperparameters {'max_depth': 3, 'max_features': 0.2}\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 3.910064697265625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.1min\n",
      "fit pipeline \t-- in 69.53558421134949s\n",
      "eval pipeline \t-- in 4.685725927352905s\n",
      "\t\t[*] fold 1 complete (test score of 5.539664643508516) -- in 74.221s\n",
      "split data \t-- in 7.152557373046875e-07s\n",
      "made pipeline \t-- in 5.984306335449219e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.5min\n",
      "fit pipeline \t-- in 158.148992061615s\n",
      "eval pipeline \t-- in 4.747580051422119s\n",
      "\t\t[*] fold 2 complete (test score of 60.74438925380716) -- in 162.897s\n",
      "split data \t-- in 6.9141387939453125e-06s\n",
      "made pipeline \t-- in 0.00010895729064941406s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.0min\n",
      "fit pipeline \t-- in 246.62568402290344s\n",
      "eval pipeline \t-- in 4.901871919631958s\n",
      "\t\t[*] fold 3 complete (test score of 11.25070936392074) -- in 251.528s\n",
      "\t\t[+] final score for params of 25.844921087078802 -- in 488.646s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 3, 'max_features': 0.4}\n",
      "split data \t-- in 5.245208740234375e-06s\n",
      "made pipeline \t-- in 2.5987625122070312e-05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.3min\n",
      "fit pipeline \t-- in 136.45146584510803s\n",
      "eval pipeline \t-- in 4.620685815811157s\n",
      "\t\t[*] fold 1 complete (test score of 5.119549133237805) -- in 141.072s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 7.104873657226562e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.0min\n",
      "fit pipeline \t-- in 304.25723910331726s\n",
      "eval pipeline \t-- in 4.612330913543701s\n",
      "\t\t[*] fold 2 complete (test score of 58.03706147059534) -- in 308.87s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.00011801719665527344s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.8min\n",
      "fit pipeline \t-- in 475.0343408584595s\n",
      "eval pipeline \t-- in 4.952126979827881s\n",
      "\t\t[*] fold 3 complete (test score of 9.203958664697517) -- in 479.987s\n",
      "\t\t[+] final score for params of 24.120189756176888 -- in 929.929s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 3, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 2.4080276489257812e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.4min\n",
      "fit pipeline \t-- in 203.4711241722107s\n",
      "eval pipeline \t-- in 4.722409009933472s\n",
      "\t\t[*] fold 1 complete (test score of 4.917571625417035) -- in 208.194s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 8.511543273925781e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.5min\n",
      "fit pipeline \t-- in 452.73901414871216s\n",
      "eval pipeline \t-- in 4.652382850646973s\n",
      "\t\t[*] fold 2 complete (test score of 57.33032482339718) -- in 457.392s\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 0.00010228157043457031s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=11.6min\n",
      "fit pipeline \t-- in 705.4148590564728s\n",
      "eval pipeline \t-- in 4.8328118324279785s\n",
      "\t\t[*] fold 3 complete (test score of 8.781892654613722) -- in 710.248s\n",
      "\t\t[+] final score for params of 23.67659636780931 -- in 1375.833s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 3, 'max_features': 0.8}\n",
      "split data \t-- in 2.86102294921875e-06s\n",
      "made pipeline \t-- in 2.6941299438476562e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.5min\n",
      "fit pipeline \t-- in 270.842337846756s\n",
      "eval pipeline \t-- in 4.612460136413574s\n",
      "\t\t[*] fold 1 complete (test score of 4.806325610816126) -- in 275.455s\n",
      "split data \t-- in 1.9073486328125e-06s\n",
      "made pipeline \t-- in 8.487701416015625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=10.0min\n",
      "fit pipeline \t-- in 602.9834949970245s\n",
      "eval pipeline \t-- in 4.616978168487549s\n",
      "\t\t[*] fold 2 complete (test score of 56.90437135714647) -- in 607.601s\n",
      "split data \t-- in 6.9141387939453125e-06s\n",
      "made pipeline \t-- in 0.00011420249938964844s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.7s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=15.5min\n",
      "fit pipeline \t-- in 937.2332129478455s\n",
      "eval pipeline \t-- in 4.8490309715271s\n",
      "\t\t[*] fold 3 complete (test score of 8.739216441500192) -- in 942.082s\n",
      "\t\t[+] final score for params of 23.483304469820933 -- in 1825.138s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 3, 'max_features': 1.0}\n",
      "split data \t-- in 1.1920928955078125e-05s\n",
      "made pipeline \t-- in 2.288818359375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.6min\n",
      "fit pipeline \t-- in 338.48403787612915s\n",
      "eval pipeline \t-- in 4.621928930282593s\n",
      "\t\t[*] fold 1 complete (test score of 4.805592809515093) -- in 343.106s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 7.700920104980469e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=12.5min\n",
      "fit pipeline \t-- in 753.3480570316315s\n",
      "eval pipeline \t-- in 4.685220956802368s\n",
      "\t\t[*] fold 2 complete (test score of 56.38007116735386) -- in 758.033s\n",
      "split data \t-- in 4.76837158203125e-06s\n",
      "made pipeline \t-- in 0.00011301040649414062s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=19.3min\n",
      "fit pipeline \t-- in 1168.0100588798523s\n",
      "eval pipeline \t-- in 4.834427833557129s\n",
      "\t\t[*] fold 3 complete (test score of 9.141618585749452) -- in 1172.845s\n",
      "\t\t[+] final score for params of 23.4424275208728 -- in 2273.984s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 4, 'max_features': 0.2}\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 2.6941299438476562e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 1.5min\n",
      "fit pipeline \t-- in 92.2739999294281s\n",
      "eval pipeline \t-- in 5.440583229064941s\n",
      "\t\t[*] fold 1 complete (test score of 5.310547044875063) -- in 97.715s\n",
      "split data \t-- in 1.9073486328125e-06s\n",
      "made pipeline \t-- in 8.511543273925781e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.4min\n",
      "fit pipeline \t-- in 208.21870398521423s\n",
      "eval pipeline \t-- in 5.278310775756836s\n",
      "\t\t[*] fold 2 complete (test score of 58.27152261974485) -- in 213.497s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 9.918212890625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.3min\n",
      "fit pipeline \t-- in 325.3078308105469s\n",
      "eval pipeline \t-- in 5.602433204650879s\n",
      "\t\t[*] fold 3 complete (test score of 10.422361702522968) -- in 330.91s\n",
      "\t\t[+] final score for params of 24.66814378904763 -- in 642.123s\n",
      "\t[*] looking at hyperparameters {'max_depth': 4, 'max_features': 0.4}\n",
      "split data \t-- in 6.9141387939453125e-06s\n",
      "made pipeline \t-- in 3.504753112792969e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.0min\n",
      "fit pipeline \t-- in 182.1832959651947s\n",
      "eval pipeline \t-- in 5.285999059677124s\n",
      "\t\t[*] fold 1 complete (test score of 4.906644884324047) -- in 187.469s\n",
      "split data \t-- in 1.9073486328125e-06s\n",
      "made pipeline \t-- in 8.20159912109375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.7min\n",
      "fit pipeline \t-- in 405.5929720401764s\n",
      "eval pipeline \t-- in 5.252935886383057s\n",
      "\t\t[*] fold 2 complete (test score of 57.00540071179742) -- in 410.846s\n",
      "split data \t-- in 5.245208740234375e-06s\n",
      "made pipeline \t-- in 0.00010514259338378906s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=10.4min\n",
      "fit pipeline \t-- in 631.920156955719s\n",
      "eval pipeline \t-- in 5.457882881164551s\n",
      "\t\t[*] fold 3 complete (test score of 8.296327209377784) -- in 637.378s\n",
      "\t\t[+] final score for params of 23.40279093516642 -- in 1235.694s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 4, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 2.574920654296875e-05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.5min\n",
      "fit pipeline \t-- in 270.2167909145355s\n",
      "eval pipeline \t-- in 5.227441072463989s\n",
      "\t\t[*] fold 1 complete (test score of 4.722455614898975) -- in 275.444s\n",
      "split data \t-- in 7.152557373046875e-07s\n",
      "made pipeline \t-- in 0.00014901161193847656s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=10.0min\n",
      "fit pipeline \t-- in 602.5553648471832s\n",
      "eval pipeline \t-- in 5.221750974655151s\n",
      "\t\t[*] fold 2 complete (test score of 56.03820142165766) -- in 607.777s\n",
      "split data \t-- in 5.9604644775390625e-06s\n",
      "made pipeline \t-- in 0.0001201629638671875s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=15.5min\n",
      "fit pipeline \t-- in 935.989245891571s\n",
      "eval pipeline \t-- in 5.451368093490601s\n",
      "\t\t[*] fold 3 complete (test score of 7.8622730722293115) -- in 941.441s\n",
      "\t\t[+] final score for params of 22.874310036261985 -- in 1824.663s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 4, 'max_features': 0.8}\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 2.8848648071289062e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.0min\n",
      "fit pipeline \t-- in 360.23419404029846s\n",
      "eval pipeline \t-- in 5.275389909744263s\n",
      "\t\t[*] fold 1 complete (test score of 4.6284533712647935) -- in 365.51s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 8.296966552734375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=13.3min\n",
      "fit pipeline \t-- in 801.1045260429382s\n",
      "eval pipeline \t-- in 5.212166786193848s\n",
      "\t\t[*] fold 2 complete (test score of 55.49547423993355) -- in 806.317s\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 0.00010991096496582031s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=20.6min\n",
      "fit pipeline \t-- in 1246.7366588115692s\n",
      "eval pipeline \t-- in 5.503493070602417s\n",
      "\t\t[*] fold 3 complete (test score of 7.617603300355581) -- in 1252.24s\n",
      "\t\t[+] final score for params of 22.58051030385131 -- in 2424.067s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 4, 'max_features': 1.0}\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 2.4080276489257812e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.5min\n",
      "fit pipeline \t-- in 450.54779028892517s\n",
      "eval pipeline \t-- in 5.395758867263794s\n",
      "\t\t[*] fold 1 complete (test score of 4.589734135733222) -- in 455.944s\n",
      "split data \t-- in 1.1920928955078125e-06s\n",
      "made pipeline \t-- in 9.703636169433594e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=16.6min\n",
      "fit pipeline \t-- in 1001.0062868595123s\n",
      "eval pipeline \t-- in 5.208301782608032s\n",
      "\t\t[*] fold 2 complete (test score of 55.4976577233098) -- in 1006.215s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.00011014938354492188s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.7s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=25.8min\n",
      "fit pipeline \t-- in 1555.9476099014282s\n",
      "eval pipeline \t-- in 5.4327380657196045s\n",
      "\t\t[*] fold 3 complete (test score of 7.627457178974852) -- in 1561.38s\n",
      "\t\t[+] final score for params of 22.571616346005957 -- in 3023.539s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 7, 'max_features': 0.2}\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 2.6702880859375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 2.6min\n",
      "fit pipeline \t-- in 154.51494812965393s\n",
      "eval pipeline \t-- in 6.987117767333984s\n",
      "\t\t[*] fold 1 complete (test score of 4.909579700302034) -- in 161.502s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 9.679794311523438e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.7min\n",
      "fit pipeline \t-- in 346.03944087028503s\n",
      "eval pipeline \t-- in 6.809956789016724s\n",
      "\t\t[*] fold 2 complete (test score of 57.029053668868634) -- in 352.85s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.00010395050048828125s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 9.2min\n",
      "fit pipeline \t-- in 558.744509935379s\n",
      "eval pipeline \t-- in 7.085996150970459s\n",
      "\t\t[*] fold 3 complete (test score of 8.891261974140424) -- in 565.831s\n",
      "\t\t[+] final score for params of 23.60996511443703 -- in 1080.183s\n",
      "\t[*] looking at hyperparameters {'max_depth': 7, 'max_features': 0.4}\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 4.315376281738281e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.7min\n",
      "fit pipeline \t-- in 285.70024394989014s\n",
      "eval pipeline \t-- in 6.5650341510772705s\n",
      "\t\t[*] fold 1 complete (test score of 4.585623430528722) -- in 292.265s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 7.915496826171875e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=10.6min\n",
      "fit pipeline \t-- in 639.707277059555s\n",
      "eval pipeline \t-- in 6.395289897918701s\n",
      "\t\t[*] fold 2 complete (test score of 55.857355528703906) -- in 646.103s\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 0.0001049041748046875s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=18.1min\n",
      "fit pipeline \t-- in 1097.0569410324097s\n",
      "eval pipeline \t-- in 6.828591823577881s\n",
      "\t\t[*] fold 3 complete (test score of 7.250769716098614) -- in 1103.886s\n",
      "\t\t[+] final score for params of 22.56458289177708 -- in 2042.254s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 7, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 2.5033950805664062e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.7min\n",
      "fit pipeline \t-- in 405.3057999610901s\n",
      "eval pipeline \t-- in 6.472675085067749s\n",
      "\t\t[*] fold 1 complete (test score of 4.479639498999273) -- in 411.779s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 9.608268737792969e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=15.1min\n",
      "fit pipeline \t-- in 911.7614469528198s\n",
      "eval pipeline \t-- in 6.28705620765686s\n",
      "\t\t[*] fold 2 complete (test score of 55.530248389531714) -- in 918.049s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.00014710426330566406s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=27.1min\n",
      "fit pipeline \t-- in 1636.1539850234985s\n",
      "eval pipeline \t-- in 6.7811689376831055s\n",
      "\t\t[*] fold 3 complete (test score of 6.512134133671259) -- in 1642.935s\n",
      "\t\t[+] final score for params of 22.17400734073408 -- in 2972.763s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 7, 'max_features': 0.8}\n",
      "split data \t-- in 4.76837158203125e-06s\n",
      "made pipeline \t-- in 3.600120544433594e-05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 8.3min\n",
      "fit pipeline \t-- in 500.0364599227905s\n",
      "eval pipeline \t-- in 6.308195114135742s\n",
      "\t\t[*] fold 1 complete (test score of 4.383803540728859) -- in 506.345s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 9.298324584960938e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=18.1min\n",
      "fit pipeline \t-- in 1090.5057940483093s\n",
      "eval pipeline \t-- in 5.986254930496216s\n",
      "\t\t[*] fold 2 complete (test score of 55.20331428751878) -- in 1096.492s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.00011372566223144531s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=36.1min\n",
      "fit pipeline \t-- in 2175.3381288051605s\n",
      "eval pipeline \t-- in 6.714541912078857s\n",
      "\t\t[*] fold 3 complete (test score of 6.422966557726807) -- in 2182.053s\n",
      "\t\t[+] final score for params of 22.003361461991485 -- in 3784.89s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 7, 'max_features': 1.0}\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 3.314018249511719e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 9.6min\n",
      "fit pipeline \t-- in 575.7400181293488s\n",
      "eval pipeline \t-- in 6.115777969360352s\n",
      "\t\t[*] fold 1 complete (test score of 4.362975803621171) -- in 581.856s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 9.894371032714844e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=20.9min\n",
      "fit pipeline \t-- in 1256.5392739772797s\n",
      "eval pipeline \t-- in 5.745612144470215s\n",
      "\t\t[*] fold 2 complete (test score of 55.31362849293387) -- in 1262.285s\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 0.00014591217041015625s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=45.1min\n",
      "fit pipeline \t-- in 2713.512176990509s\n",
      "eval pipeline \t-- in 6.749855279922485s\n",
      "\t\t[*] fold 3 complete (test score of 6.5254466495474475) -- in 2720.262s\n",
      "\t\t[+] final score for params of 22.067350315367495 -- in 4564.404s\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'max_features': 0.2}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 3.910064697265625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 3.3min\n",
      "fit pipeline \t-- in 200.41323804855347s\n",
      "eval pipeline \t-- in 7.95060396194458s\n",
      "\t\t[*] fold 1 complete (test score of 4.755156597046078) -- in 208.364s\n",
      "split data \t-- in 9.5367431640625e-07s\n",
      "made pipeline \t-- in 0.0001201629638671875s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.4min\n",
      "fit pipeline \t-- in 448.0610280036926s\n",
      "eval pipeline \t-- in 7.816370248794556s\n",
      "\t\t[*] fold 2 complete (test score of 56.628337750862016) -- in 455.878s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 0.0001430511474609375s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=12.6min\n",
      "fit pipeline \t-- in 767.6013813018799s\n",
      "eval pipeline \t-- in 8.32932710647583s\n",
      "\t\t[*] fold 3 complete (test score of 8.254057065395182) -- in 775.931s\n",
      "\t\t[+] final score for params of 23.21251713776776 -- in 1440.173s\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'max_features': 0.4}\n",
      "split data \t-- in 6.198883056640625e-06s\n",
      "made pipeline \t-- in 3.814697265625e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 5.4min\n",
      "fit pipeline \t-- in 327.89599418640137s\n",
      "eval pipeline \t-- in 7.06255316734314s\n",
      "\t\t[*] fold 1 complete (test score of 4.525979460446512) -- in 334.959s\n",
      "split data \t-- in 2.86102294921875e-06s\n",
      "made pipeline \t-- in 0.0003452301025390625s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=12.1min\n",
      "fit pipeline \t-- in 732.8701961040497s\n",
      "eval pipeline \t-- in 6.909559011459351s\n",
      "\t\t[*] fold 2 complete (test score of 55.80395770608907) -- in 739.78s\n",
      "split data \t-- in 3.0994415283203125e-06s\n",
      "made pipeline \t-- in 0.0003180503845214844s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=22.9min\n",
      "fit pipeline \t-- in 1384.5577471256256s\n",
      "eval pipeline \t-- in 7.659032821655273s\n",
      "\t\t[*] fold 3 complete (test score of 6.71268703148139) -- in 1392.217s\n",
      "\t\t[+] final score for params of 22.347541399338994 -- in 2466.956s\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 4.220008850097656e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.1min\n",
      "fit pipeline \t-- in 425.9848849773407s\n",
      "eval pipeline \t-- in 6.650087356567383s\n",
      "\t\t[*] fold 1 complete (test score of 4.417833095729572) -- in 432.635s\n",
      "split data \t-- in 1.1920928955078125e-06s\n",
      "made pipeline \t-- in 0.00017404556274414062s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=15.9min\n",
      "fit pipeline \t-- in 957.4894351959229s\n",
      "eval pipeline \t-- in 6.464565992355347s\n",
      "\t\t[*] fold 2 complete (test score of 55.32248317889691) -- in 963.954s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.0001671314239501953s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=30.5min\n",
      "fit pipeline \t-- in 1835.7836537361145s\n",
      "eval pipeline \t-- in 7.2879931926727295s\n",
      "\t\t[*] fold 3 complete (test score of 6.074529290138628) -- in 1843.072s\n",
      "\t\t[+] final score for params of 21.9382818549217 -- in 3239.661s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'max_features': 0.8}\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 3.600120544433594e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 8.4min\n",
      "fit pipeline \t-- in 505.08971190452576s\n",
      "eval pipeline \t-- in 6.479978084564209s\n",
      "\t\t[*] fold 1 complete (test score of 4.338252429021781) -- in 511.57s\n",
      "split data \t-- in 1.9073486328125e-06s\n",
      "made pipeline \t-- in 0.00023508071899414062s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=18.2min\n",
      "fit pipeline \t-- in 1096.4093861579895s\n",
      "eval pipeline \t-- in 6.021513223648071s\n",
      "\t\t[*] fold 2 complete (test score of 55.071793122001765) -- in 1102.431s\n",
      "split data \t-- in 5.245208740234375e-06s\n",
      "made pipeline \t-- in 0.00019931793212890625s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   8.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=38.5min\n",
      "fit pipeline \t-- in 2316.050888299942s\n",
      "eval pipeline \t-- in 7.429027795791626s\n",
      "\t\t[*] fold 3 complete (test score of 6.148583273109156) -- in 2323.48s\n",
      "\t\t[+] final score for params of 21.852876274710898 -- in 3937.482s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 10, 'max_features': 1.0}\n",
      "split data \t-- in 7.152557373046875e-06s\n",
      "made pipeline \t-- in 4.38690185546875e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=10.0min\n",
      "fit pipeline \t-- in 602.2416598796844s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval pipeline \t-- in 9.49461817741394s\n",
      "\t\t[*] fold 1 complete (test score of 4.295536555265822) -- in 611.736s\n",
      "split data \t-- in 7.152557373046875e-06s\n",
      "made pipeline \t-- in 0.0007369518280029297s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   6.8s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=28.1min\n",
      "fit pipeline \t-- in 1694.9324769973755s\n",
      "eval pipeline \t-- in 8.110289096832275s\n",
      "\t\t[*] fold 2 complete (test score of 55.09430116701679) -- in 1703.044s\n",
      "split data \t-- in 1.0967254638671875e-05s\n",
      "made pipeline \t-- in 0.0006229877471923828s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=  10.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing model, total=185.3min\n",
      "fit pipeline \t-- in 11130.189529895782s\n",
      "eval pipeline \t-- in 7.83368706703186s\n",
      "\t\t[*] fold 3 complete (test score of 6.158764446155101) -- in 11138.024s\n",
      "\t\t[+] final score for params of 21.849534056145902 -- in 13452.805s\n",
      "\t\t[!] new best param configuration, so saving model to path\n",
      "\t\t\t[+] saved model to ../results/RandomForestRegressor(1:01 on 3-12-2023).pkl\n",
      "\t[*] looking at hyperparameters {'max_depth': 14, 'max_features': 0.2}\n",
      "split data \t-- in 1.3113021850585938e-05s\n",
      "made pipeline \t-- in 6.604194641113281e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 4.1min\n",
      "fit pipeline \t-- in 248.17979216575623s\n",
      "eval pipeline \t-- in 9.976490020751953s\n",
      "\t\t[*] fold 1 complete (test score of 4.7369222147304155) -- in 258.156s\n",
      "split data \t-- in 4.0531158447265625e-06s\n",
      "made pipeline \t-- in 0.0007870197296142578s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   6.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 9.4min\n",
      "fit pipeline \t-- in 567.2869958877563s\n",
      "eval pipeline \t-- in 10.401787996292114s\n",
      "\t\t[*] fold 2 complete (test score of 56.25443841184725) -- in 577.69s\n",
      "split data \t-- in 9.298324584960938e-06s\n",
      "made pipeline \t-- in 0.0006241798400878906s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   9.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=49.1min\n",
      "fit pipeline \t-- in 2953.8924691677094s\n",
      "eval pipeline \t-- in 10.327025890350342s\n",
      "\t\t[*] fold 3 complete (test score of 8.38512436452929) -- in 2964.22s\n",
      "\t\t[+] final score for params of 23.12549499703565 -- in 3800.067s\n",
      "\t[*] looking at hyperparameters {'max_depth': 14, 'max_features': 0.4}\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 6.890296936035156e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.8s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.0min\n",
      "fit pipeline \t-- in 360.4174258708954s\n",
      "eval pipeline \t-- in 8.806205987930298s\n",
      "\t\t[*] fold 1 complete (test score of 4.523076907738564) -- in 369.224s\n",
      "split data \t-- in 5.0067901611328125e-06s\n",
      "made pipeline \t-- in 0.0004999637603759766s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=13.6min\n",
      "fit pipeline \t-- in 824.1649787425995s\n",
      "eval pipeline \t-- in 9.044985055923462s\n",
      "\t\t[*] fold 2 complete (test score of 56.048726384288315) -- in 833.21s\n",
      "split data \t-- in 4.291534423828125e-06s\n",
      "made pipeline \t-- in 0.0004990100860595703s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   9.5s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=43.4min\n",
      "fit pipeline \t-- in 2610.5576832294464s\n",
      "eval pipeline \t-- in 8.06781792640686s\n",
      "\t\t[*] fold 3 complete (test score of 6.761291708324512) -- in 2618.626s\n",
      "\t\t[+] final score for params of 22.44436500011713 -- in 3821.061s\n",
      "\t[*] looking at hyperparameters {'max_depth': 14, 'max_features': 0.6000000000000001}\n",
      "split data \t-- in 3.814697265625e-06s\n",
      "made pipeline \t-- in 5.1021575927734375e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total= 7.1min\n",
      "fit pipeline \t-- in 427.46301198005676s\n",
      "eval pipeline \t-- in 7.315906047821045s\n",
      "\t\t[*] fold 1 complete (test score of 4.402312850481388) -- in 434.779s\n",
      "split data \t-- in 5.245208740234375e-06s\n",
      "made pipeline \t-- in 0.0004811286926269531s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=15.8min\n",
      "fit pipeline \t-- in 954.9258308410645s\n",
      "eval pipeline \t-- in 7.101922988891602s\n",
      "\t\t[*] fold 2 complete (test score of 55.563972399822106) -- in 962.028s\n",
      "split data \t-- in 4.76837158203125e-06s\n",
      "made pipeline \t-- in 0.0005459785461425781s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   9.2s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=31.6min\n",
      "fit pipeline \t-- in 1906.4690520763397s\n",
      "eval pipeline \t-- in 7.538548946380615s\n",
      "\t\t[*] fold 3 complete (test score of 5.956386052895506) -- in 1914.008s\n",
      "\t\t[+] final score for params of 21.974223767732997 -- in 3310.816s\n",
      "\t[*] looking at hyperparameters {'max_depth': 14, 'max_features': 0.8}\n",
      "split data \t-- in 1.1920928955078125e-05s\n",
      "made pipeline \t-- in 6.079673767089844e-05s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   1.6s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=11.0min\n",
      "fit pipeline \t-- in 658.6904110908508s\n",
      "eval pipeline \t-- in 7.731701850891113s\n",
      "\t\t[*] fold 1 complete (test score of 4.321171566492993) -- in 666.422s\n",
      "split data \t-- in 1.2874603271484375e-05s\n",
      "made pipeline \t-- in 0.0008809566497802734s\n",
      "[Pipeline] ........ (step 1 of 2) Processing preprocess, total=   5.8s\n"
     ]
    }
   ],
   "source": [
    "RF_PARAM_GRID = ParameterGrid({\n",
    "            'max_depth': [1, 3, 4, 7, 10, 14, 21, 31], # no upper bound so the values are evenly spaced in log\n",
    "            'max_features': np.linspace(0.2, 1, 5)\n",
    "            })\n",
    "RF_PARAM_GRID_SMALL = ParameterGrid({\n",
    "            'max_depth': [1, 3, 7, 14, 31], # no upper bound so the values are evenly spaced in log\n",
    "            'max_features': np.linspace(0.2, 1, 3)\n",
    "            })\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, RandomForestRegressor, RF_PARAM_GRID)\n",
    "\n",
    "ELASTIC_PARAM_GRID = {\n",
    "            'alpha': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000], # no upper bound so the values are evenly spaced in log\n",
    "            'l1_ratio': np.linspace(0.2, 1, 9)\n",
    "            }\n",
    "\n",
    "MLPipe_TimeSeries_RMSE(X, y, preprocessor, ElasticNet, ELASTIC_PARAM_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82921452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93183abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6324bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_partial = X_test.sample(np.floor((len(X_test) * 0.3)).astype(int))\n",
    "y_test_partial = y_test.loc[X_test_partial.index]\n",
    "y_test_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = []\n",
    "y_trains = []\n",
    "X_vals = []\n",
    "y_vals = []\n",
    "for i in range(1, 3 + 1):\n",
    "    train_portion = np.round(i * VAL_PORTION, 2)\n",
    "    div_0 = new_quarters[0]\n",
    "    div_1 = new_quarters[int(quarter_count * train_portion)]\n",
    "    div_2 = new_quarters[int(quarter_count * (train_portion + VAL_PORTION))]\n",
    "\n",
    "    X_trains.append(X[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    y_trains.append(y[(div_0 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_1)])\n",
    "    X_vals.append(X[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])\n",
    "    y_vals.append(y[(div_1 <= X[\"TIME_PERIOD\"]) & (X[\"TIME_PERIOD\"] < div_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8a7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_best = np.argmin(param_scores)\n",
    "param_scores[i_best]\n",
    "param_grid[i_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929dfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_algo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc3dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68176db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"../results/Ridge(19:57 on 2-12-2023).pkl\")\n",
    "cfs = pd.DataFrame(model.named_steps[\"model\"].coef_)\n",
    "cfs.index = model.named_steps['preprocess'].get_feature_names_out()\n",
    "cfs[0].sort_values(ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5da48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old split checking\n",
    "def check_split_sizes(X, train, test, val):\n",
    "    fails = 0\n",
    "    print(\"[*] checking train test val split\")\n",
    "    train_set_qs = set(train[\"TIME_PERIOD\"])\n",
    "    test_set_qs = set(test[\"TIME_PERIOD\"])\n",
    "    val_set_qs = set(val[\"TIME_PERIOD\"])\n",
    "    \n",
    "    #check for TIME_PERIOD overlap\n",
    "    shared = (train_set_qs & test_set_qs) | (val_set_qs & test_set_qs) | (train_set_qs & val_set_qs)\n",
    "    if (len(shared) != 0):\n",
    "        warnings.warn('\\t[-] overlap between train, test, or val time_periods')\n",
    "        fails+=1\n",
    "    else:\n",
    "        print(\"\\t[+] no overlap between train, test, or val TIME_PERIODS\")\n",
    "        \n",
    "    #check for a fairly even 60/20/20 split\n",
    "    NAMES = ['train', 'test ', 'val  ']\n",
    "    TARGETS = [0.6, 0.2, 0.2]\n",
    "    ALLOWED_FRACTION_ERROR = 0.02\n",
    "    sizes = [len(train) / len(X), len(test) / len(X), len(val) / len(X)]\n",
    "    for i in range(0, 3):\n",
    "        if (np.abs(sizes[i] - TARGETS[i]) < ALLOWED_FRACTION_ERROR):\n",
    "            print(\"\\t[+] \" + str(NAMES[i]) + \" is \" + str(np.round(sizes[i], 3)) + \" of datapoints which is within bounds of its \" + str(TARGETS[i]) + \" target\")\n",
    "        else:\n",
    "            warnings.warn(\"\\t[-] \" + str(NAMES[i]) + \" is \" + str(np.round(sizes[i], 3)) + \" of datapoints which is out of bounds\")\n",
    "            fails+=1\n",
    "\n",
    "    if (fails == 0):\n",
    "        print(\"\\t[+] \\x1b[42mPASSED ALL\\x1b[0m train test val split tests\")\n",
    "    else:\n",
    "        print(\"\\t[?] \\033[91mFAILED \" + str(fails) + \"\\033[0m train test val split tests\")\n",
    "        \n",
    "\n",
    "#check_split_sizes(X, X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d982fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdadb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30ce7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25946758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b8071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d532059",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650c131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************feature scaling********************************************************\n",
    "\n",
    "ordinal_ftrs = ['age', 'TIME_PERIOD']\n",
    "ordinal_cats = [['UNK','Y_LT14','Y14-17','Y18-34','Y35-64','Y_GE65'], quarters]\n",
    "                                                                     #^^i'm using quarters not new_quarters here so that\n",
    "                                                                     #  the model can still tell where in history this q is\n",
    "onehot_ftrs = ['citizen', 'geo', 'sex']\n",
    "#onehot_ftrs = ['geo', 'sex']\n",
    "minmax_ftrs = []\n",
    "std_ftrs = [a for a in X.columns.to_list() if 'TOTAL' in a]\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "#X_train_prep = clf.fit_transform(X_train)\n",
    "#X_val_prep = clf.transform(X_val)\n",
    "#X_test_prep = clf.transform(X_test)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train_prep.shape)\n",
    "#print(X_train_prep)\n",
    "#X_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d55328c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1c98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab34edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
